[{"content":"Por lo general, cuando comenzamos nuestra jornada laboral desde la primera hora de la ma√±ana, ya nos sentimos un poco agobiados y nos abruman la cantidad de correos electr√≥nicos que tenemos en la bandeja de entrada, las tareas asignadas que tenemos que desarrollar o alguna presentaci√≥n que tenemos que finalizar.\nEs por eso que me gustar√≠a compartir contigo 3 t√©cnicas que voy alternando y utilizo a diario en mi d√≠a a d√≠a, y que me ayudan enormemente a completar todas las tareas que tengo pendientes para el d√≠a sin perder el enfoque.\n Getting Things Done (GTD). Tiempo por bloques (Time blocking). T√©cnica Pomodoro.  1. T√©cnica GTD (Getting Things Done):\nGTD parte del principio de que todas las tareas se deben registrar por escrito, por m√°s que sean obvias o sencillas de realizar. De esta manera, liberas a tu cerebro de recordarlas y te enfocas en lo importante. Entonces:\n üì§ Recopilar: Registra cualquier cosa que cruce tu mente. ¬°Nada es demasiado grande o peque√±o! Esto ayuda a liberar tu mente de preocupaciones y a tener un lugar centralizado donde puedas registrar todo lo que necesitas hacer. ‚úèÔ∏è Procesar: Una vez que has recopilado todas las cosas en tu lista, debes procesarlas una por una. Decide qu√© hacer con cada una de ellas: si es algo que puedes hacer en menos de dos minutos, hazlo de inmediato. Si no, dec√≠delo: delega, pospone o archiva la tarea en una lista de proyectos o acciones futuras. üîÄ Organizar: Clasifica tus tareas y proyectos en categor√≠as y contextos espec√≠ficos para que puedas abordarlas de manera m√°s eficiente. Utiliza listas, etiquetas y sistemas de organizaci√≥n que te ayuden a acceder a la informaci√≥n de manera r√°pida y sencilla. üîé Evaluar: Realiza revisiones peri√≥dicas de tus listas y proyectos para asegurarte de que est√°s trabajando en las tareas m√°s importantes y relevantes. Esto te ayuda a mantener el enfoque en lo que realmente importa. üí™üèª Hacer: Finalmente, lleva a cabo las tareas de acuerdo a tus prioridades. Elige lo que hacer en funci√≥n de tu disponibilidad, energ√≠a y contexto.  2. T√©cnica de Tiempo por Bloques (Time Blocking):\n Time blocking es un m√©todo que consiste en dividir tu d√≠a en bloques de tiempo. En cada bloque, te dedicas a realizar una tarea espec√≠fica o un grupo de tareas. En lugar de mantener una lista de tareas pendientes abierta de cosas a las que llegar√°s cuando puedas, comienzas cada d√≠a con un horario concreto que describe en qu√© trabajar√°s y cu√°ndo. La clave de este m√©todo es priorizar tu lista de tareas con anticipaci√≥n; una revisi√≥n semanal dedicada es imprescindible. Toma nota de lo que viene para la pr√≥xima semana y haz un bosquejo aproximado de tus bloques de tiempo para cada d√≠a. Al final de cada jornada laboral, revisa cualquier tarea que no hayas terminado, as√≠ como cualquier nueva tarea que haya entrado, y ajusta tus bloques de tiempo para el resto de la semana en consecuencia. Con d√≠as que se han bloqueado a tiempo de antemano, no tendr√°s que tomar decisiones constantemente sobre a qu√© enfocarte. Todo lo que necesitas hacer es seguir tu horario de tiempo bloqueado. Si te distraes o te desv√≠as de la tarea, simplemente mira tu horario y vuelve a la tarea para la que bloqueaste tiempo.  3. T√©cnica Pomodoro:\nLa t√©cnica Pomodoro es un enfoque de gesti√≥n del tiempo desarrollado por Francesco Cirillo a finales de la d√©cada de 1980. La t√©cnica se basa en la idea de dividir el tiempo en intervalos de trabajo concentrado y descanso. Aqu√≠ hay una descripci√≥n b√°sica de c√≥mo funciona:\n Elije una tarea que deseas completar. Configura un temporizador para 25 minutos y comienza a trabajar en la tarea seleccionada. A este intervalo se le llama \u0026ldquo;pomodoro\u0026rdquo;. Trabaja de manera intensa y enfocada en la tarea hasta que suene el temporizador. Una vez que se complete un pomodoro, toma un breve descanso de 5 minutos para relajarte, estirarte o hacer algo diferente. Despu√©s de completar cuatro pomodoros (o cuatro intervalos de 25 minutos), toma un descanso m√°s largo de 15-30 minutos.  Lo que encuentro positivo de esta t√©cnica es que te proporciona una estructura para dividir tu trabajo en tareas manejables con frecuentes descansos cortos para promover la concentraci√≥n sostenida durante todo el d√≠a y evitar la fatiga mental y visual.\nEspero que estas estrategias te sean de utilidad. Dejame en los comentarios sobre c√≥mo te funcionan a ti!\nY recuerda, si hay algo que se puede decir sobre el trabajo en estos tiempos, es esto: si no controlas tu horario, √©l te controlar√° a ti.üôÇ\n","permalink":"https://elvisfinol.com/posts/3-tecnicas-para-mejorar-tu-productividad/","summary":"Por lo general, cuando comenzamos nuestra jornada laboral desde la primera hora de la ma√±ana, ya nos sentimos un poco agobiados y nos abruman la cantidad de correos electr√≥nicos que tenemos en la bandeja de entrada, las tareas asignadas que tenemos que desarrollar o alguna presentaci√≥n que tenemos que finalizar.\nEs por eso que me gustar√≠a compartir contigo 3 t√©cnicas que voy alternando y utilizo a diario en mi d√≠a a d√≠a, y que me ayudan enormemente a completar todas las tareas que tengo pendientes para el d√≠a sin perder el enfoque.","title":"3 T√©cnicas Para Mejorar Tu Productividad."},{"content":"Your AWS credentials let you log into the AWS management console, manage services, view and edit resources, and so on. Security in AWS begins with the foundation of identity, which is managed by the Identity and Access Management (IAM) service. Because your AWS credentials are the keys to the kingdom, the first order of business is to protect them from accidental exposure and unauthorised use. The second step is to ensure that users have only the permissions they need, and no more.\n IAM is used to securely control individual and group access to AWS resources. IAM makes it easy to provide multiple users secure access to AWS resources. IAM can be used to manage:  Users. Groups. Access policies. Roles. User credentials. User password policies. Multi-factor authentication (MFA). API keys for programmatic access (CLI).   Provides centralized control of your AWS account. Enables shared access to your AWS account. By default new users are created with NO access to any AWS services ‚Äì they can only login to the AWS console. Permission must be explicitly granted to allow a user to access an AWS service. IAM users are individuals who have been granted access to an AWS account. Each IAM user has three main components:  A user-name. A password. Permissions to access various resources.   You can apply granular permissions with IAM. You can assign users individual security credentials such as access keys, passwords, and multi-factor authentication devices. IAM is not used for application-level authentication. Identity Federation (including AD, Facebook etc.) can be configured allowing secure access to resources in an AWS account without creating an IAM user account. Multi-factor authentication (MFA) can be enabled/enforced for the AWS account and for individual users under the account. MFA uses an authentication device that continually generates random, six-digit, single-use authentication codes. You can authenticate using an MFA device in the following three ways:  Through the AWS Management Console ‚Äì the user is prompted for a user name, password and authentication code. Using the AWS API ‚Äì restrictions are added to IAM policies and developers can request temporary security credentials and pass MFA parameters in their AWS STS API requests. Using the AWS CLI by obtaining temporary security credentials from STS (aws sts get-session-token).   It is a best practice to use MFA for all users and to use U2F or hardware MFA devices for all privileged users. IAM is universal (global) and does not apply to regions. IAM is eventually consistent. IAM replicates data across multiple data centres around the world. The ‚Äúroot account‚Äù is the account created when you setup the AWS account. It has complete Admin access and is the only account that has this access by default. It is a best practice to not use the root account for anything other than billing. Power user access allows all permissions except the management of groups and users in IAM. Temporary security credentials consist of the AWS access key ID, secret access key, and security token. IAM can assign temporary security credentials to provide users with temporary access to services/resources. IAM integrates with many different AWS services. IAM supports PCI DSS compliance. AWS recommend that you use the AWS SDKs to make programmatic API calls to IAM.   References\n https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/ AWS Certified Solutions Architect Study Guide: Associate SAA-CO2 Exam  ","permalink":"https://elvisfinol.com/posts/iam-aws/","summary":"Your AWS credentials let you log into the AWS management console, manage services, view and edit resources, and so on. Security in AWS begins with the foundation of identity, which is managed by the Identity and Access Management (IAM) service. Because your AWS credentials are the keys to the kingdom, the first order of business is to protect them from accidental exposure and unauthorised use. The second step is to ensure that users have only the permissions they need, and no more.","title":"IAM - AWS"},{"content":"Functions of some core AWS services, organized by category.\nCompute Elastic Compute Cloud (EC2)\nEC2 server instances provide virtual versions of the servers you would run in your local data center. EC2 instances can be provisioned with the CPU, memory, storage, and network interface profile to meet any application need, from a simple web server to one part of a cluster of instances providing an integrated multi-tiered fleet architecture. Since EC2 instances are virtual, they‚Äôre resource-efficient and deploy nearly instantly.\nLambda\nServerless application architectures like the one provided by Amazon‚Äôs Lambda service allow you to provide responsive public-facing services without the need for\na server that‚Äôs actually running 24/7. Instead, network events (like consumer requests) can trigger the execution of a predefined code-based operation. When the operation (which can currently run for as long as 15 minutes) is complete, the Lambda event ends, and all resources automatically shut down.\nAuto Scaling\nCopies of running EC2 instances can be defined as image templates and automatically launched (or scaled up) when client demand can‚Äôt be met by existing instances. As demand drops, unused instances can be terminated (or scaled down).\nElastic Load Balancing\nIncoming network traffic can be directed between multiple web servers to ensure that a single web server isn‚Äôt overwhelmed while other servers are underused or that traffic isn‚Äôt directed to failed servers.\nNetworking Elastic Beanstalk\nBeanstalk is a managed service that abstracts the provisioning of AWS compute and networking infrastructure. You are required to do nothing more than push your application code, and Beanstalk automatically launches and manages all the necessary services in the background.\nVirtual Private Cloud (VPC)\nVPCs are highly configurable networking environments designed to host your EC2 (and RDS) instances. You use VPC-based tools to secure and, if desired, isolate your instances by closely controlling inbound and outbound network access.\nDirect Connect\nBy purchasing fast and secure network connections to AWS through a third-party provider, you can use Direct Connect to establish an enhanced direct tunnel between your local data center or office and your AWS-based VPCs.\nRoute 53\nRoute 53 is the AWS DNS service that lets you manage domain registration, record administration, routing protocols, and health checks, which are all fully integrated with the rest of your AWS resources.\nCloudFront\nCloudFront is Amazon‚Äôs distributed global content delivery network (CDN). When properly configured, a CloudFront distribution can store cached versions of your site‚Äôs content at edge locations around the world so that they can be delivered to customers on request with the greatest efficiency and lowest latency.\nStorage Simple Storage Service (S3)\nS3 offers highly versatile, reliable, and inexpensive object storage that‚Äôs great for data storage and backups. It‚Äôs also commonly used as part of larger AWS production processes, including through the storage of script, template, and log files.\nS3 Glacier\nA good choice for when you need large data archives stored cheaply over the long term and can live with retrieval delays measuring in the hours. Glacier‚Äôs lifecycle management is closely integrated with S3.\nElastic Block Store (EBS)\nEBS provides the persistent virtual storage drives that host the operating systems and working data of an EC2 instance. They‚Äôre meant to mimic the function of the storage drives and partitions attached to physical servers.\nStorage Gateway\nStorage Gateway is a hybrid storage system that exposes AWS cloud storage as a local, on-premises appliance. Storage Gateway can be a great tool for migration and data backup and as part of disaster recovery operations.\nDatabase Relational Database Service (RDS)\nRDS is a managed service that builds you a stable, secure, and reliable database instance. You can run a variety of SQL database engines on RDS, including MySQL, Microsoft SQL Server, Oracle, and Amazon‚Äôs own Aurora.\nDynamoDB\nDynamoDB can be used for fast, flexible, highly scalable, and managed nonrelational (NoSQL) database workloads.\nApplication Management CloudWatch\nCloudWatch can be set to monitor process performance and resource utilization and, when preset thresholds are met, either send you a message or trigger an automated response.\nCloudFormation\nThis service enables you to use template files to define full and complex AWS deployments. The ability to script your use of any AWS resources makes it easier to automate, standardising and speeding up the application launch process.\nCloudTrail\nCloudTrail collects records of all your account‚Äôs API events. This history is useful for account auditing and troubleshooting purposes.\nConfig\nThe Config service is designed to help you with change management and compliance for your AWS account. You first define a desired configuration state, and Config evaluates any future states against that ideal. When a configuration change pushes too far from the ideal baseline, you‚Äôll be notified.\nSecurity and identity Identity and Access Management (IAM)\nYou use IAM to administrate user and programmatic access and authentication to your AWS account. Through the use of users, groups, roles, and policies, you can control exactly who and what can access and/or work with any of your AWS resources.\nKey Management Service (KMS)\nKMS is a managed service that allows you to administrate the creation and use of encryption keys to secure data used by and for any of your AWS resources.\nDirectory Service\nFor AWS environments that need to manage identities and relationships, Directory Service can integrate AWS resources with identity providers like Amazon Cognito and Microsoft AD domains.\nApplication integration Simple Notification Service (SNS)\nSNS is a notification tool that can automate the publishing of alert topics to other services (to an SQS Queue or to trigger a Lambda function, for instance), to mobile devices, or to recipients using email or SMS.\nSimple Workflow (SWF)\nSWF lets you coordinate a series of tasks that must be performed using a range of AWS services or even non- digital (meaning, human) events.\nSimple Queue Service (SQS)\nSQS allows for event-driven messaging within distributed systems that can decouple while coordinating the discrete steps of a larger process. The data contained in your SQS messages will be reliably delivered, adding to the fault tolerant qualities of an application.\nAPI Gateway\nThis service enables you to create and manage secure and reliable APIs for your AWS-based applications.\n References\n AWS Certified Solutions Architect Study Guide: Associate SAA-CO2 Exam  ","permalink":"https://elvisfinol.com/posts/core-aws-services/","summary":"Functions of some core AWS services, organized by category.\nCompute Elastic Compute Cloud (EC2)\nEC2 server instances provide virtual versions of the servers you would run in your local data center. EC2 instances can be provisioned with the CPU, memory, storage, and network interface profile to meet any application need, from a simple web server to one part of a cluster of instances providing an integrated multi-tiered fleet architecture. Since EC2 instances are virtual, they‚Äôre resource-efficient and deploy nearly instantly.","title":"Core AWS services"},{"content":"Paso a paso Descarga el paquete binario de Terraform apropiado para la m√°quina virtual de tu servidor, en este caso lo har√© para Linux de 64 bits mediante el comando wget:\nwget -c https://releases.hashicorp.com/terraform/0.13.4/terraform_0.13.4_linux_amd64.zip  Descomprime el archivo descargado:\nunzip terraform_0.13.4_linux_amd64.zip  Coloca el binario de Terraform en el PATH del sistema operativo de la m√°quina virtual para que el binario sea accesible en todo el sistema para todos los usuarios:\nsudo mv terraform /usr/sbin/   Nota: Si se te solicita, ingrese el nombre de usuario y la contrase√±a de tu servidor.\n Verifica la versi√≥n de Terraform:\nterraform version  Si devuelve la versi√≥n de Terraform, el binario de Terraform est√° instalado y funciona correctamente.\nClonar sobre c√≥digo para proveedores de Terraform Crea un directorio de proveedores:\nmkdir providers  Dir√≠gete al directorio de proveedores:\ncd providers/  Crea el archivo main.tf:\nvim main.tf  Pega el siguiente c√≥digo proporcionado\nprovider \u0026quot;aws\u0026quot; { alias = \u0026quot;us-east-1\u0026quot; region = \u0026quot;us-east-1\u0026quot; } provider \u0026quot;aws\u0026quot; { alias = \u0026quot;us-west-2\u0026quot; region = \u0026quot;us-west-2\u0026quot; } resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic-us-east\u0026quot; { provider = aws.us-east-1 name = \u0026quot;topic-us-east\u0026quot; } resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic-us-west\u0026quot; { provider = aws.us-west-2 name = \u0026quot;topic-us-west\u0026quot; }  Para guardar y salir del archivo, presiona Escape e ingresa: wq!\nImplementar el c√≥digo con Terraform Apply Habilita el registro de salida detallado para los comandos de Terraform usando TF_LOG = TRACE:\nexport TF_LOG=TRACE   Nota: Puedes desactivar el registro detallado en cualquier momento mediante el comando export TF_LOG =.\n Inicializa el directorio de trabajo donde se encuentra el c√≥digo:\nterraform init  Revisa las acciones realizadas cuando implementa el c√≥digo Terraform:\nterraform plan  Nota: Se crear√°n dos recursos, de acuerdo con los proveedores que se configuraron en el fragmento de c√≥digo proporcionado.\nImplementa el c√≥digo:\nterraform apply  Cuando se te solicite, escribe YES y presiona Enter.\nVerifica que se hayan creado dos recursos con sus ID de nombre de recursos de Amazon (ARN) correspondientes en la regi√≥n en la que se activaron.\nOpcionalmente, verifica que los recursos se crearon en sus respectivas regiones dentro de la Consola de administraci√≥n de AWS\nElimina la infraestructura que acaba de crear:\nterraform destroy --auto-approve ","permalink":"https://elvisfinol.com/posts/installing-terraform-and-working-with-terraform-providers/","summary":"Paso a paso Descarga el paquete binario de Terraform apropiado para la m√°quina virtual de tu servidor, en este caso lo har√© para Linux de 64 bits mediante el comando wget:\nwget -c https://releases.hashicorp.com/terraform/0.13.4/terraform_0.13.4_linux_amd64.zip  Descomprime el archivo descargado:\nunzip terraform_0.13.4_linux_amd64.zip  Coloca el binario de Terraform en el PATH del sistema operativo de la m√°quina virtual para que el binario sea accesible en todo el sistema para todos los usuarios:","title":"Instalaci√≥n de Terraform Linux 64 bit - AWS"},{"content":"In May 2021, I went through an interview process at AWS for the role of Solutions Architect. Unfortunately, I did not successfully pass the interview loop. However, I did receive good feedback about my assignment. Therefore, I would like to share my experience with you regarding this process and discuss the assignment in detail. I won‚Äôt upload the Cloud Formation template for obvious reasons, but the exercise is not complicated, and it allows you to be creative on the assignment.\n Consider that you are doing this for a customer, so make sure to put in some effort.\n By the way, I‚Äôm not certain if this assignment is still current, but it can be helpful for practice if you‚Äôre preparing the interview for the AWS Solutions Architect role.\nContext So far your customer has launched an AWS Elastic Load Balancer (ELB) and an Amazon Elastic Compute Cloud (EC2) instance acting as the web server. Both are deployed in a Virtual Private Cloud (VPC) on AWS. While your customer\u0026rsquo;s initial deployment aims to present a static web page to its users (demo.html located in the document root of the web server), the end solution should continue to be suitable for generating dynamic responses (your customer is currently developing the application). The customer is not sure about their future direction or requirements and are looking to you to provide expert guidance despite the ambiguity.\nYou are contacted and asked to:\na) Troubleshoot the implementation by doing the minimum amount of work required to make the website operational. Your customer expects detailed written troubleshooting instructions or scripts for the in-house team.\nStep by step to find the problem (Questions that I made)\n Verify the topology used on the CloudFormation template Verify VPC/EC2 configuration:   IGW is associated with the VPC? Route tables are associate with the instance subnets? Does the instance have Public IP associated? Validate Instance status checks.  Verify Elastic Load Balancer configuration   Is the instance attached to the ELB? Does ELB is pointing to the correct AZ? How is the health check configured?  Verify Inbound/outbound rules configured on SecurityGroups   Does the ELB SG have rules configured? Does the APP SG have rules configured?  b) Propose short term changes you could help them implement to improve the availability, security, reliability, cost and performance before the project goes into production. Your customer expects you to explain the business and technical benefits of your proposals, with artifacts such as a design or architecture document and diagrams.\nReviewing the Cloud Formation stack you have provided; These recommendations will help you to easy scale your application, having workloads reliable, built-in security and highly available, taking consideration on costs. You will find a few new AWS services in this proposal, however, will explain all advantages and main purposes.\nI propose a three tier web application architecture (Web/Presentation, App/Logic and Data separately). Each tier will be independent of the other tiers, so updates or changes can be carried out without affecting the application as a whole.\n First entry service I recommend Route53 for DNS Resolution to connects user requests to AWS infrastructure. You do not have to pay upfront fees and pay just for what you use, you will be charged for the amount resolved queries. For more information, see https://aws.amazon.com/route53/pricing/?nc1=h_ls Each Route53 health check can monitor a specified resource of your architecture, being able to trigger an email alarm, so you be always up to date if something failed. The NACLs (Network Access Control List) on your template has default configuration allowing inbound/outbound to all type of traffic. So define the type of requests your app will receive and always ensure you do not use a wide range of ports or overly permissions to NACLs during configuration. Use NACLs in combination with your Security Groups inside your VPC to reinforce the security. As the entry point of your VPC there is an Application Load Balancer to route all requests at the application layer of your web server locate in the public subnet. ALB perform health checks and guarantee all petitions will be delegated only to the healthy instance. Also adding a security layer to your instances due it restricts the web servers accessible only via the ALB. Configure your ALB to listen only HTTPS instead of HTTP for secure purpose. Make sure to change your web server configuration and configure your SSL/TLS certificate. I recommend add an auto-scaling group (minimum 1 instance running) to your Web Server to protect the instance from interruptions, ensuring the minimum number of instances and responds to changes in load by scaling the instance pool appropriately. If your user requests increase, your web server can scale automatically and servers will manage the increase load. Same recommendation on your private subnet that your app tier (under development) will be deployed. Auto-scaling group will manage the scalability on those servers. You can try with spot instances (to reduce costs) at the logic tier until you define your workload and the integration with your Presentation Layer. Finally, at the Data tier I select a generic database. Could be a Relational or NoSQL. Not every database fits every business. If you have a schema define and data integrity is essential for you, go with SQL database. In case, your data has no schema define, no need for joins or complex transactions and scale faster a NoSQL database would fit better. For costs, create a billing alarm based on your monthly budget. I would recommend use AWS budget to define custom budgets that alert you when your costs or usage exceed and use AWS Cost Explorer to manage your costs and usage over time providing fine-grained billing to understand your costs.  c) Optionally, propose high level alternative solution(s) for the longer term as their web application becomes more successful.\n Considering the short term architecture, I proposed keeping the three tier and launching infrastructure on 2 AWS Availability zones. To provide high availability. In case of issues you can route to your healthy AZ. I would recommend adding CDN like CloudFront to cache the static website content from your web server (Presentation layer). Also, CloudFront will offer you low latency, high transfer speeds and security capabilities. You do not have to pass all the internet route, instead you are entering through the AWS backbone. In front of the CloudFront I suggest using a Web Application Firewall (AWS WAF) to filter malicious requests and protect from DDoS attacks. I would add an extra ALB to distribute load for your Web tier, and your App tier solution. It may give you an extra cost however you decouple your app reducing dependencies between and allow you to can scale faster. In the long run would be better. As mentioned above on short term (Web Server and App Servers), will keep the use of auto-scaling to automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. In case of high demand your compute resources will adapt to the amount of requests. For example, you could scale based on CPU usage. For more information, see https://aws.amazon.com/es/autoscaling/features/ You could use EBS volumes for your EC2 instances and perform snapshots monthly to keep you OS backup. For the Data tier, if you use relational database like RDS you can set up Multi AZ with auto-failover or use Aurora which is a MySQL and PostgreSQL-compatible relational database built for the cloud and is highly available by nature. In case of a NoSQL database I would recommend DynamoDB providing speed, scalability and synchronously replication across availability zones. For monitoring your complete stack (applications, infrastructure, and services) and centralize your logging solution, use CloudWatch. It also helps you to understand the infrastructure. Use S3 (Object storage solution service) so all the logs will be saved in one place for investigation if something fail. S3 has several storage classes, to rotate logs and reduce costs. Use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. You will gain granular control and enhanced security.  Hope you find this useful, and good luck in your upcoming interview. üöÄ\n","permalink":"https://elvisfinol.com/posts/aws-solutions-architect-assignment-for-the-loop/","summary":"In May 2021, I went through an interview process at AWS for the role of Solutions Architect. Unfortunately, I did not successfully pass the interview loop. However, I did receive good feedback about my assignment. Therefore, I would like to share my experience with you regarding this process and discuss the assignment in detail. I won‚Äôt upload the Cloud Formation template for obvious reasons, but the exercise is not complicated, and it allows you to be creative on the assignment.","title":"AWS Solutions Architect - Assignment for the loop!"},{"content":"What is DevOps? DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality.\nBuild Automation What is Build Automation?  Build automation: automation of the process of preparing code for deployment to a live environment Depending on what languages are used, code needs to be compiled, linted, minified, transformed, unit tested, etc. Build automation means taking these steps and doing them in a consistent, automated way using a script or tool The tools of build automation often differ depending on what programming languages and frameworks are used, but they have one thing in common: automation!  What does Build Automation look like?  Usually, build automation looks like running a command-line tool that builds code using configuration files and/or scripts that are treated as part of the source code Build automation is independent of an IDE Even if you can build within the IDE, it should be able to work the same way outside the IDE As much as possible, build automation should be agnostic of the configuration of the machine that it is build on Your code should be able to build on someone else\u0026rsquo;s machine the same way it builds on yours  Why do Build Automation?  Fast - Automation handle tasks that would otherwise need to be done manually Consistency - The build happens the same way every time, removing problems and confusion that happen with manual builds Repeatable - The build can be done multiple times with the same results. Any version of the code can always be transformed into deployable code in a consistent way Portable - The build can be done the same way on any machine. Anyone on the team can be build on their machine, as well as on a shared build server. Building code doesn\u0026rsquo;t depend on specific people or machines Reliable - There will be fewer problems caused by bad builds  Continuous Integration What is Continuous Integration?  Continuous Integration (CI): the practice of frequently merging code done by developers Traditionally, developers would work separately, perhaps for weeks at a time, and the merge all their work together at the end in one large effort CI means merging constantly thought out the day, usually with the execution of automated tests to detect any problems caused by the merge Merging all the time could be a lot of work, so to avoid that it should be automated  What does Continuous Integration look like?  Continuous integration is usually done with the help of a CI server When a developer commits a code change, the CI server sees the change and automatically perform the build, also executing automated tests This occurs multiple times a day If there is any problem with the build, the CI server immediately and automatically notifies the developers If anyone commits code that \u0026ldquo;breaks the build\u0026rdquo; they are responsible for fixing the problem or rolling back their changes immediately so that other developers can continue working  Why do Continuous Integration?  Early detection of certain types of bugs - If code doesn\u0026rsquo;t compile or an automated test fails, the developers are notified and can fix it immediately. The sooner these bugs are detected, the easier they are to fix Eliminate the scramble to integrate just before a big release - The code is constantly merged, so there is no need to do a big merge at the end Make frequent releases possible - Code is always in a state that can be deployed to production Makes continuous testing possible - Since the code can always be run, QA testers can get their hands on it all throughout the development process, not just at the end Encourages good coding practices - Frequent commits encourages simple, modular code  Continuous Delivery and Continuous Deployment What is Continuous Delivery?  Continuous Delivery (CD): the practice of continuously maintaining code in a deployable state Regardless of whether or not the decision is made to deploy, the code is always in a state that is able to be deployed Some terms Continuous Delivery and Continuous Deployment interchangeably, or simply use the abbreviation CD  What is Continuous Deployment?  Continuous Deployment: the practice of frequently deploying small code changes to production Continuous Delivery is keeping the code in a deployable state. Continuous Delivery is actually doing the deployment frequently Some companies that do Continuous Deployment deploy to production multiple times a day There is no standard for how often you should deploy, but in general the more often you deploy the better With Continuous Deployment, deployments to production are routine and commonplace. They are not a big, scary event  What does Continuous Delivery and Continuous Deployment look like?  Each version of the code goes through a series of states, such as automated build, automated testing and manual acceptance testing. The result of this process is an artifact or package that is able to be deployed When the decision is made to deploy, the deployment is automated. What the automated deployment looks like depends on the architecture, but no matter what the architecture is, the deployment is automated If a deployment causes a problem, it is quickly and reliably rolled back using and automated process Rollbacks aren\u0026rsquo;t a big deal because the developers can redeploy a fixed version as soon as they have one available No one grips their desk in fear during a deployment, even if the deployment does cause a problem  Why do Continuous Delivery and Continuous Deployment?  Faster time-to-market - Get features into the hands of customers more quickly rather than waiting for a lengthy deployment process that doesn\u0026rsquo;t happen often Fewer problems caused by the deployment process - Since the deployment process is frequently used, any problems with the process are mo easily discovered Lower risk - The more changes are deployed at once, the higher the risk. Frequent deployments of only a few changes are less risky Reliable rollbacks - Robust automation means rollbacks are a reliable way to ensure stability for customers, and rollback don\u0026rsquo;t hurt developers because they can roll forward with a fix as soon as they have one Fearless deployments - Robust automation plus the ability to rollback quickly means deployments are commonplace, everyday events rather than big, scary events  Infrastructure as Code What is Infrastructure as Code?  Infrastructure as Code (IaC): manage and provision infrastructure through code and automation With infrastructure as code, instead of doing things manually, you use automation and code to create and change  Servers Instances Environments Containers Other infrastructure    What does infrastructure as code look like?  Without infrastructure as code, you might:  SSH into a host Issues a series of commands to perform the change   With infrastructure as code:  Change some code or configuration files that can be used with an automation tool to perform changes Commit them to source control Use an automation tool to enact the changes defined in the code and/or configuration files   With Infrastructure as code, provisioning new resources and changing existing resources are both done through automation  Why do infrastructure as code?  Consistency in creation and management of resources - The same automation will run the same way every time Reusability - Code can be used to make the same change consistently across multiple hosts and can be used again in the future Scalability - Need a new instance? You can have one configured exactly the same way as the existing instances in minutes (or seconds) Self-documenting - With IaC, changes to infrastructure documents themselves to a degree. The way a server is configured can be viewed in source control, rather than being a matter of who logged in to the server and did something Simplify the complexity - Complex infrastructures can be stood up quickly once they are defined as code. A group of several interdependent servers can be provisioned on demand  Configuration Management What is Configuration Management?  Configuration Management: maintaining and changing the state of pieces of infrastructure in a consistent, maintainable and stable way Changes always need to happen - configuration management is about doing them in a maintainable way Configuration management allows you to minimize configuration drift - the small changes that accumulate over time and make systems different from one another and harder to manage Infrastructure as Code is very beneficial for configuration management  What does Configuration Management look like?  You need to upgrade a software package on a bunch of servers:  Without good configuration management, you log into each server and perform the upgrade. However, this can lead to a lot of problems. Perhaps one server was missed due to poor documentation, or perhaps something doesn‚Äôt work while the versions are temporarily mismatched between servers, causing a lot of downtime while you do the upgrade With good configuration management, you define the new version of the software package in a configuration file or tool and automatically roll out the change to all the servers   Configuration Management is about managing your configuration somewhere outside the servers themselves  Why do Configuration Management?  Save time - It takes less time to change the configuration Insight - With good configuration management, you can know about the state of all pieces of a large and complex infrastructure Maintainability - A more maintainable infrastructure is easier to change in a stable way Less configuration drift - It is easier to keep a standard configuration across a multitude of hosts  Orchestration What is Orchestration?  Automation that supports processes and workflows, such as provisioning resources With Orchestration, managing a complex infrastructure is less like being a builder and more like conducting an orchestra Instead of going out and creating a piece of infrastructure, the conductor simply signals what needs to be done and be orchestra performs it:  The conductor does not need to control every detail The musicions (automation) are able to perfrom their piece with only a little bit of guidance    What does Orchestration look like?  Here is an example:  A customer requests more resources for a web service that is about to see a heavy increase in usage due to a planned marketing effort Instead of manually standing up new nodes, operations enginieers use an orchestration tool to request five more nodes to support the service A few minutes later, the tool has five new nodes are up and running   A much cooler example:  A monitoring tool detects an increased load on the service An orchestration tool responds to this by spinning up additional resources to handle the load When the load decreases again, the tool spins the additional resources back down, freeing them up to be used by something else All of this happens while the engineer is getting coffee    Why do Orchestration?  Scalability - Resources can be quickly increased or decreased to meet changing needs Stability - Automation tools can automatically respond to fix problems before users see them Save time - Certain tasks and workflows can be automated, freeing up engineers' time Selt-service - Orchestration can be used to offer resources to customers in a seft-service fashion Granular insight into resource usage - Orchestration tools give greater insight into how many resources are being used by what software, services or customers  Monitoring What is Monitoring?  Monitoring: The collection and presentation of data about the performance and stability of services and infrastructure Monitoring tools collect data over things such as:  Usage of memory CPU Disk I/O Other resources over time Application Logs Network traffic etc.   The collected data is presented in various forms, such as charts and graphs, or in the form of real-time notifications about problems.  What does Monitoring look like?  Real-time notifications:  Performance on the website is beginning to slow down A monitoring tool detects that response times are growing An administrator is immediately notified and is able to interviene before downtime occurs   Postmortem analysis:  Something went wrong in production last night It\u0026rsquo;s working now, but we don\u0026rsquo;t know what caused it Luckily, monitoring tools collected a lot of data during the outage Will the data, developers and operations engineers are able to determine the root cause (a poorly performing SQL query) and fix it    Why do Monitoring?  Fast recovery - The sooner a problem is detected, the sooner it can be fixed. You want to know about a problem before your customer does Better root cause analysis - The more data you have, the easier it is to determine the root cause of a problem Visibility across teams - Good monitoring tools give useful data to both developers and operations people about the performance of code in production Automated response - Monitoring data can be used alongside orchestration to provide  Microservices What are Microservices?  A microservice architecture breaks an application up into a collection of small loosely-coupled services Traditionally, apps used a monolithic architecture, In a monolithic architecture, all features and services are part of one large application Microservices are small: each microservice implements only a small piece of an application\u0026rsquo;s overall functionality Microservices are loosely coupled: Different microservices interact with each other using stable and well-defined APIs. this means that they are independent of one another  What do microservices look like?  For example, a pet shop application might have:  A pet inventory service A customer details service An authentication service A pet adoption request service A payment processing service   Each of these is its own codebase and separate running process (or processes). They can all be build, deployed, and scaled separately  Why use Microservices?  Modularity - Microservices encourage modularity. In monolithic apps, individual pieces become tightly coupled, and complexity grows. Eventually, It\u0026rsquo;s very hard to change anything without breaking something Technological flexibility - You don\u0026rsquo;t need to use the same languages and technologies for every part of the app. You can use the best tool for each job Optimized scalability - You can scale individual parts of the app based upon resources usage and load. With monolithic, you have to scale up the entire application, even if only one aspect of the service actually needs to be scaled Microservices aren\u0026rsquo;t always the best choice. For small, simpler apps a monilith might be easier to manage  ","permalink":"https://elvisfinol.com/posts/devops-concepts/","summary":"What is DevOps? DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous delivery with high software quality.\nBuild Automation What is Build Automation?  Build automation: automation of the process of preparing code for deployment to a live environment Depending on what languages are used, code needs to be compiled, linted, minified, transformed, unit tested, etc.","title":"DevOps Concepts"},{"content":"You can use this script to install/update HUGO (static site generator) on your macOS\nSteps\nCopy the script and save it on your machine as hugo_latest.sh\n# hugo_latest.sh # Find the latest Hugo from GitHub echo 'üêπ Starting Hugo Install / Update üêπ' echo ' Note: Please be sure to have curl and grep installed' echo '' url=$(curl -s \u0026quot;https://api.github.com/repositories/11180687/releases/latest\u0026quot; | grep -o 'https://.*hugo_extended.*_macOS-64bit.tar.gz') echo '‚úÖ Found latest version' curl -s $url -L -o hugo_latest.tar.gz echo '‚úÖ Download complete: ' $url tar -zxf hugo_latest.tar.gz -C /usr/local/bin rm /usr/local/bin/README.md rm /usr/local/bin/LICENSE echo '‚úÖ Extracted to /usr/local/bin' rm hugo_latest.tar.gz echo '‚úÖ Removed downloaded artifacts' echo '' echo 'üëâ Current Version' $(hugo version) echo '' echo 'üéâüéâüéâ Happy Hugo-ing! üéâüéâüéâ  Open the Terminal on you macOS, locate the file and run the script:\n./hugo_latest.sh   Do not forget run \u0026ldquo;chmod\u0026rdquo; to give read permission\n Wait until it finish, it may take a while. Validate the HUGO version running the following command:\nhugo version hugo v0.88.0-ACC5EB5B+extended darwin/amd64 BuildDate=2021-09-02T09:27:28Z VendorInfo=gohugoio  Done! :)\n","permalink":"https://elvisfinol.com/posts/script-to-install-hugo-latest-version-for-macos/","summary":"You can use this script to install/update HUGO (static site generator) on your macOS\nSteps\nCopy the script and save it on your machine as hugo_latest.sh\n# hugo_latest.sh # Find the latest Hugo from GitHub echo 'üêπ Starting Hugo Install / Update üêπ' echo ' Note: Please be sure to have curl and grep installed' echo '' url=$(curl -s \u0026quot;https://api.github.com/repositories/11180687/releases/latest\u0026quot; | grep -o 'https://.*hugo_extended.*_macOS-64bit.tar.gz') echo '‚úÖ Found latest version' curl -s $url -L -o hugo_latest.","title":"Script to install/update HUGO on your macOS"},{"content":"SEARCHING ALGORITHMS BINARY SEARCH binarySearch(input, searchValue) { lower = 0 upper = input.len - 1 white(upper \u0026gt;= lower) { mid = (upper + lower) / 2 if (input[mid] == searchValue) return True elif (searchValue \u0026lt; input[mid]) upper = mid - 1 else: low = mid + 1 } return false } LINEAR SEARCH linearSearch(input, searchValue) { for (i=0 to input.lenght - 1) { if (input[i] == searchValue) return true } return false } JUMP SEARCH # Python3 code to implement Jump Search import math def jumpSearch( arr , x , n ): # Finding block size to be jumped step = math.sqrt(n) # Finding the block where element is # present (if it is present) prev = 0 while arr[int(min(step, n)-1)] \u0026lt; x: prev = step step += math.sqrt(n) if prev \u0026gt;= n: return -1 # Doing a linear search for x in # block beginning with prev. while arr[int(prev)] \u0026lt; x: prev += 1 # If we reached next block or end # of array, element is not present. if prev == min(step, n): return -1 # If element is found if arr[int(prev)] == x: return prev return -1 # Driver code to test function arr = [ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610 ] x = 55 n = len(arr) # Find the index of 'x' using Jump Search index = jumpSearch(arr, x, n) # Print the index where 'x' is located print(\u0026quot;Number\u0026quot; , x, \u0026quot;is at index\u0026quot; ,\u0026quot;%.0f\u0026quot;%index) SORTING ALGORITHMS BUBBLE SORT # Python program for implementation of Bubble Sort def bubbleSort(arr): n = len(arr) # Traverse through all array elements for i in range(n-1): # range(n) also work but outer loop will repeat one time more than needed. # Last i elements are already in place for j in range(0, n-i-1): # traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if arr[j] \u0026gt; arr[j+1] : arr[j], arr[j+1] = arr[j+1], arr[j] # Driver code to test above arr = [64, 34, 25, 12, 22, 11, 90] bubbleSort(arr) print (\u0026quot;Sorted array is:\u0026quot;) for i in range(len(arr)): print (\u0026quot;%d\u0026quot; %arr[i]), INSERTION SORT # Python program for implementation of Insertion Sort # Function to do insertion sort def insertionSort(arr): # Traverse through 1 to len(arr) for i in range(1, len(arr)): key = arr[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j \u0026gt;=0 and key \u0026lt; arr[j] : arr[j+1] = arr[j] j -= 1 arr[j+1] = key # Driver code to test above arr = [12, 11, 13, 5, 6] insertionSort(arr) print (\u0026quot;Sorted array is:\u0026quot;) for i in range(len(arr)): print (\u0026quot;%d\u0026quot; %arr[i]) SELECTION SORT # Python program for implementation of Selection # Sort import sys A = [64, 25, 12, 22, 11] # Traverse through all array elements for i in range(len(A)): # Find the minimum element in remaining # unsorted array min_idx = i for j in range(i+1, len(A)): if A[min_idx] \u0026gt; A[j]: min_idx = j # Swap the found minimum element with # the first element\tA[i], A[min_idx] = A[min_idx], A[i] # Driver code to test above print (\u0026quot;Sorted array\u0026quot;) for i in range(len(A)): print(\u0026quot;%d\u0026quot; %A[i]), HEAP SORT # Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap def heapify(arr, n, i): largest = i # Initialize largest as root l = 2 * i + 1\t# left = 2*i + 1 r = 2 * i + 2\t# right = 2*i + 2 # See if left child of root exists and is # greater than root if l \u0026lt; n and arr[i] \u0026lt; arr[l]: largest = l # See if right child of root exists and is # greater than root if r \u0026lt; n and arr[largest] \u0026lt; arr[r]: largest = r # Change root, if needed if largest != i: arr[i],arr[largest] = arr[largest],arr[i] # swap # Heapify the root. heapify(arr, n, largest) # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. # Since last parent will be at ((n//2)-1) we can start at that location. for i in range(n // 2 - 1, -1, -1): heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # swap heapify(arr, i, 0) # Driver code to test above arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) n = len(arr) print (\u0026quot;Sorted array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), QUICK SORT # Python program for implementation of Quicksort Sort # This function takes last element as pivot, places # the pivot element at its correct position in sorted # array, and places all smaller (smaller than pivot) # to left of pivot and all greater elements to right # of pivot def partition(arr, low, high): i = (low-1)\t# index of smaller element pivot = arr[high]\t# pivot for j in range(low, high): # If current element is smaller than or # equal to pivot if arr[j] \u0026lt;= pivot: # increment index of smaller element i = i+1 arr[i], arr[j] = arr[j], arr[i] arr[i+1], arr[high] = arr[high], arr[i+1] return (i+1) # The main function that implements QuickSort # arr[] --\u0026gt; Array to be sorted, # low --\u0026gt; Starting index, # high --\u0026gt; Ending index # Function to do Quick sort def quickSort(arr, low, high): if len(arr) == 1: return arr if low \u0026lt; high: # pi is partitioning index, arr[p] is now # at right place pi = partition(arr, low, high) # Separately sort elements before # partition and after partition quickSort(arr, low, pi-1) quickSort(arr, pi+1, high) # Driver code to test above arr = [10, 7, 8, 9, 1, 5] n = len(arr) quickSort(arr, 0, n-1) print(\u0026quot;Sorted array is:\u0026quot;) for i in range(n): print(\u0026quot;%d\u0026quot; % arr[i]), MERGE SORT # Python program for implementation of MergeSort # Merges two subarrays of arr[]. # First subarray is arr[l..m] # Second subarray is arr[m+1..r] def merge(arr, l, m, r): n1 = m - l + 1 n2 = r- m # create temp arrays L = [0] * (n1) R = [0] * (n2) # Copy data to temp arrays L[] and R[] for i in range(0 , n1): L[i] = arr[l + i] for j in range(0 , n2): R[j] = arr[m + 1 + j] # Merge the temp arrays back into arr[l..r] i = 0\t# Initial index of first subarray j = 0\t# Initial index of second subarray k = l\t# Initial index of merged subarray while i \u0026lt; n1 and j \u0026lt; n2 : if L[i] \u0026lt;= R[j]: arr[k] = L[i] i += 1 else: arr[k] = R[j] j += 1 k += 1 # Copy the remaining elements of L[], if there # are any while i \u0026lt; n1: arr[k] = L[i] i += 1 k += 1 # Copy the remaining elements of R[], if there # are any while j \u0026lt; n2: arr[k] = R[j] j += 1 k += 1 # l is for left index and r is right index of the # sub-array of arr to be sorted def mergeSort(arr,l,r): if l \u0026lt; r: # Same as (l+r)//2, but avoids overflow for # large l and h m = (l+(r-1))//2 # Sort first and second halves mergeSort(arr, l, m) mergeSort(arr, m+1, r) merge(arr, l, m, r) # Driver code to test above arr = [12, 11, 13, 5, 6, 7] n = len(arr) print (\u0026quot;Given array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), mergeSort(arr,0,n-1) print (\u0026quot;\\n\\nSorted array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), ","permalink":"https://elvisfinol.com/posts/algorithms-pseudo-codes/","summary":"SEARCHING ALGORITHMS BINARY SEARCH binarySearch(input, searchValue) { lower = 0 upper = input.len - 1 white(upper \u0026gt;= lower) { mid = (upper + lower) / 2 if (input[mid] == searchValue) return True elif (searchValue \u0026lt; input[mid]) upper = mid - 1 else: low = mid + 1 } return false } LINEAR SEARCH linearSearch(input, searchValue) { for (i=0 to input.lenght - 1) { if (input[i] == searchValue) return true } return false } JUMP SEARCH # Python3 code to implement Jump Search import math def jumpSearch( arr , x , n ): # Finding block size to be jumped step = math.","title":"Algorithms Pseudocode In Python üß¨"},{"content":"Bash is a shell program. A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n Note: A binary is a file that contains the instructions for a program, ie it is a ‚Äòprogram‚Äô file, rather than a ‚Äòtext‚Äô file, or an ‚Äòapplication‚Äô file (such as a Word document).\n You only need to know that a shell program is a program that allows you to tell the computer what to do. In that way, it‚Äôs not much different to many other kinds of programming languages.\n#!/usr/bin/env bash # First line of the script is the shebang which tells the system how to execute # the script: https://en.wikipedia.org/wiki/Shebang_(Unix) # As you already figured, comments start with #. Shebang is also a comment. # Simple hello world example: echo Hello world! # =\u0026gt; Hello world! # Each command starts on a new line, or after a semicolon: echo 'This is the first line'; echo 'This is the second line' # =\u0026gt; This is the first line # =\u0026gt; This is the second line # Declaring a variable looks like this: Variable=\u0026quot;Some string\u0026quot; # But not like this: Variable = \u0026quot;Some string\u0026quot; # =\u0026gt; returns error \u0026quot;Variable: command not found\u0026quot; # Bash will decide that Variable is a command it must execute and give an error # because it can't be found. # Nor like this: Variable= 'Some string' # =\u0026gt; returns error: \u0026quot;Some string: command not found\u0026quot; # Bash will decide that 'Some string' is a command it must execute and give an # error because it can't be found. (In this case the 'Variable=' part is seen # as a variable assignment valid only for the scope of the 'Some string' # command.) # Using the variable: echo $Variable # =\u0026gt; Some string echo \u0026quot;$Variable\u0026quot; # =\u0026gt; Some string echo '$Variable' # =\u0026gt; $Variable # When you use the variable itself ‚Äî assign it, export it, or else ‚Äî you write # its name without $. If you want to use the variable's value, you should use $. # Note that ' (single quote) won't expand the variables! # Parameter expansion ${ }: echo ${Variable} # =\u0026gt; Some string # This is a simple usage of parameter expansion # Parameter Expansion gets a value from a variable. # It \u0026quot;expands\u0026quot; or prints the value # During the expansion time the value or parameter can be modified # Below are other modifications that add onto this expansion # String substitution in variables echo ${Variable/Some/A} # =\u0026gt; A string # This will substitute the first occurrence of \u0026quot;Some\u0026quot; with \u0026quot;A\u0026quot; # Substring from a variable Length=7 echo ${Variable:0:Length} # =\u0026gt; Some st # This will return only the first 7 characters of the value echo ${Variable: -5} # =\u0026gt; tring # This will return the last 5 characters (note the space before -5) # String length echo ${#Variable} # =\u0026gt; 11 # Indirect expansion OtherVariable=\u0026quot;Variable\u0026quot; echo ${!OtherVariable} # =\u0026gt; Some String # This will expand the value of OtherVariable # Default value for variable echo ${Foo:-\u0026quot;DefaultValueIfFooIsMissingOrEmpty\u0026quot;} # =\u0026gt; DefaultValueIfFooIsMissingOrEmpty # This works for null (Foo=) and empty string (Foo=\u0026quot;\u0026quot;); zero (Foo=0) returns 0. # Note that it only returns default value and doesn't change variable value. # Declare an array with 6 elements array0=(one two three four five six) # Print first element echo $array0 # =\u0026gt; \u0026quot;one\u0026quot; # Print first element echo ${array0[0]} # =\u0026gt; \u0026quot;one\u0026quot; # Print all elements echo ${array0[@]} # =\u0026gt; \u0026quot;one two three four five six\u0026quot; # Print number of elements echo ${#array0[@]} # =\u0026gt; \u0026quot;6\u0026quot; # Print number of characters in third element echo ${#array0[2]} # =\u0026gt; \u0026quot;5\u0026quot; # Print 2 elements starting from forth echo ${array0[@]:3:2} # =\u0026gt; \u0026quot;four five\u0026quot; # Print all elements. Each of them on new line. for i in \u0026quot;${array0[@]}\u0026quot;; do echo \u0026quot;$i\u0026quot; done # Brace Expansion { } # Used to generate arbitrary strings echo {1..10} # =\u0026gt; 1 2 3 4 5 6 7 8 9 10 echo {a..z} # =\u0026gt; a b c d e f g h i j k l m n o p q r s t u v w x y z # This will output the range from the start value to the end value # Built-in variables: # There are some useful built-in variables, like echo \u0026quot;Last program's return value: $?\u0026quot; echo \u0026quot;Script's PID: $$\u0026quot; echo \u0026quot;Number of arguments passed to script: $#\u0026quot; echo \u0026quot;All arguments passed to script: $@\u0026quot; echo \u0026quot;Script's arguments separated into different variables: $1 $2...\u0026quot; # Now that we know how to echo and use variables, # let's learn some of the other basics of bash! # Our current directory is available through the command `pwd`. # `pwd` stands for \u0026quot;print working directory\u0026quot;. # We can also use the built-in variable `$PWD`. # Observe that the following are equivalent: echo \u0026quot;I'm in $(pwd)\u0026quot; # execs `pwd` and interpolates output echo \u0026quot;I'm in $PWD\u0026quot; # interpolates the variable # If you get too much output in your terminal, or from a script, the command # `clear` clears your screen clear # Ctrl-L also works for clearing output # Reading a value from input: echo \u0026quot;What's your name?\u0026quot; read Name # Note that we didn't need to declare a new variable echo Hello, $Name! # We have the usual if structure: # use `man test` for more info about conditionals if [ $Name != $USER ] then echo \u0026quot;Your name isn't your username\u0026quot; else echo \u0026quot;Your name is your username\u0026quot; fi # True if the value of $Name is not equal to the current user's login username # NOTE: if $Name is empty, bash sees the above condition as: if [ != $USER ] # which is invalid syntax # so the \u0026quot;safe\u0026quot; way to use potentially empty variables in bash is: if [ \u0026quot;$Name\u0026quot; != $USER ] ... # which, when $Name is empty, is seen by bash as: if [ \u0026quot;\u0026quot; != $USER ] ... # which works as expected # There is also conditional execution echo \u0026quot;Always executed\u0026quot; || echo \u0026quot;Only executed if first command fails\u0026quot; # =\u0026gt; Always executed echo \u0026quot;Always executed\u0026quot; \u0026amp;\u0026amp; echo \u0026quot;Only executed if first command does NOT fail\u0026quot; # =\u0026gt; Always executed # =\u0026gt; Only executed if first command does NOT fail # To use \u0026amp;\u0026amp; and || with if statements, you need multiple pairs of square brackets: if [ \u0026quot;$Name\u0026quot; == \u0026quot;Steve\u0026quot; ] \u0026amp;\u0026amp; [ \u0026quot;$Age\u0026quot; -eq 15 ] then echo \u0026quot;This will run if $Name is Steve AND $Age is 15.\u0026quot; fi if [ \u0026quot;$Name\u0026quot; == \u0026quot;Daniya\u0026quot; ] || [ \u0026quot;$Name\u0026quot; == \u0026quot;Zach\u0026quot; ] then echo \u0026quot;This will run if $Name is Daniya OR Zach.\u0026quot; fi # There is also the `=~` operator, which tests a string against a Regex pattern: Email=me@example.com if [[ \u0026quot;$Email\u0026quot; =~ [a-z]+@[a-z]{2,}\\.(com|net|org) ]] then echo \u0026quot;Valid email!\u0026quot; fi # Note that =~ only works within double [[ ]] square brackets, # which are subtly different from single [ ]. # See https://www.gnu.org/software/bash/manual/bashref.html#Conditional-Constructs for more on this. # Redefine command `ping` as alias to send only 5 packets alias ping='ping -c 5' # Escape the alias and use command with this name instead \\ping 192.168.1.1 # Print all aliases alias -p # Expressions are denoted with the following format: echo $(( 10 + 5 )) # =\u0026gt; 15 # Unlike other programming languages, bash is a shell so it works in the context # of a current directory. You can list files and directories in the current # directory with the ls command: ls # Lists the files and subdirectories contained in the current directory # This command has options that control its execution: ls -l # Lists every file and directory on a separate line ls -t # Sorts the directory contents by last-modified date (descending) ls -R # Recursively `ls` this directory and all of its subdirectories # Results of the previous command can be passed to the next command as input. # The `grep` command filters the input with provided patterns. # That's how we can list .txt files in the current directory: ls -l | grep \u0026quot;\\.txt\u0026quot; # Use `cat` to print files to stdout: cat file.txt # We can also read the file using `cat`: Contents=$(cat file.txt) # \u0026quot;\\n\u0026quot; prints a new line character # \u0026quot;-e\u0026quot; to interpret the newline escape characters as escape characters echo -e \u0026quot;START OF FILE\\n$Contents\\nEND OF FILE\u0026quot; # =\u0026gt; START OF FILE # =\u0026gt; [contents of file.txt] # =\u0026gt; END OF FILE # Use `cp` to copy files or directories from one place to another. # `cp` creates NEW versions of the sources, # so editing the copy won't affect the original (and vice versa). # Note that it will overwrite the destination if it already exists. cp srcFile.txt clone.txt cp -r srcDirectory/ dst/ # recursively copy # Look into `scp` or `sftp` if you plan on exchanging files between computers. # `scp` behaves very similarly to `cp`. # `sftp` is more interactive. # Use `mv` to move files or directories from one place to another. # `mv` is similar to `cp`, but it deletes the source. # `mv` is also useful for renaming files! mv s0urc3.txt dst.txt # sorry, l33t hackers... # Since bash works in the context of a current directory, you might want to # run your command in some other directory. We have cd for changing location: cd ~ # change to home directory cd # also goes to home directory cd .. # go up one directory # (^^say, from /home/username/Downloads to /home/username) cd /home/username/Documents # change to specified directory cd ~/Documents/.. # still in home directory..isn't it?? cd - # change to last directory # =\u0026gt; /home/username/Documents # Use subshells to work across directories (echo \u0026quot;First, I'm here: $PWD\u0026quot;) \u0026amp;\u0026amp; (cd someDir; echo \u0026quot;Then, I'm here: $PWD\u0026quot;) pwd # still in first directory # Use `mkdir` to create new directories. mkdir myNewDir # The `-p` flag causes new intermediate directories to be created as necessary. mkdir -p myNewDir/with/intermediate/directories # if the intermediate directories didn't already exist, running the above # command without the `-p` flag would return an error # You can redirect command input and output (stdin, stdout, and stderr). # Read from stdin until ^EOF$ and overwrite hello.py with the lines # between \u0026quot;EOF\u0026quot;: cat \u0026gt; hello.py \u0026lt;\u0026lt; EOF #!/usr/bin/env python from __future__ import print_function import sys print(\u0026quot;#stdout\u0026quot;, file=sys.stdout) print(\u0026quot;#stderr\u0026quot;, file=sys.stderr) for line in sys.stdin: print(line, file=sys.stdout) EOF # Variables will be expanded if the first \u0026quot;EOF\u0026quot; is not quoted # Run the hello.py Python script with various stdin, stdout, and # stderr redirections: python hello.py \u0026lt; \u0026quot;input.in\u0026quot; # pass input.in as input to the script python hello.py \u0026gt; \u0026quot;output.out\u0026quot; # redirect output from the script to output.out python hello.py 2\u0026gt; \u0026quot;error.err\u0026quot; # redirect error output to error.err python hello.py \u0026gt; \u0026quot;output-and-error.log\u0026quot; 2\u0026gt;\u0026amp;1 # redirect both output and errors to output-and-error.log python hello.py \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # redirect all output and errors to the black hole, /dev/null, i.e., no output # The output error will overwrite the file if it exists, # if you want to append instead, use \u0026quot;\u0026gt;\u0026gt;\u0026quot;: python hello.py \u0026gt;\u0026gt; \u0026quot;output.out\u0026quot; 2\u0026gt;\u0026gt; \u0026quot;error.err\u0026quot; # Overwrite output.out, append to error.err, and count lines: info bash 'Basic Shell Features' 'Redirections' \u0026gt; output.out 2\u0026gt;\u0026gt; error.err wc -l output.out error.err # Run a command and print its file descriptor (e.g. /dev/fd/123) # see: man fd echo \u0026lt;(echo \u0026quot;#helloworld\u0026quot;) # Overwrite output.out with \u0026quot;#helloworld\u0026quot;: cat \u0026gt; output.out \u0026lt;(echo \u0026quot;#helloworld\u0026quot;) echo \u0026quot;#helloworld\u0026quot; \u0026gt; output.out echo \u0026quot;#helloworld\u0026quot; | cat \u0026gt; output.out echo \u0026quot;#helloworld\u0026quot; | tee output.out \u0026gt;/dev/null # Cleanup temporary files verbosely (add '-i' for interactive) # WARNING: `rm` commands cannot be undone rm -v output.out error.err output-and-error.log rm -r tempDir/ # recursively delete # You can install the `trash-cli` Python package to have `trash` #¬†which puts files in the system trash and doesn't delete them directly # see https://pypi.org/project/trash-cli/ if you want to be careful # Commands can be substituted within other commands using $( ): # The following command displays the number of files and directories in the # current directory. echo \u0026quot;There are $(ls | wc -l) items here.\u0026quot; # The same can be done using backticks `` but they can't be nested - # the preferred way is to use $( ). echo \u0026quot;There are `ls | wc -l` items here.\u0026quot; # Bash uses a `case` statement that works similarly to switch in Java and C++: case \u0026quot;$Variable\u0026quot; in # List patterns for the conditions you want to meet 0) echo \u0026quot;There is a zero.\u0026quot;;; 1) echo \u0026quot;There is a one.\u0026quot;;; *) echo \u0026quot;It is not null.\u0026quot;;; #¬†match everything esac # `for` loops iterate for as many arguments given: # The contents of $Variable is printed three times. for Variable in {1..3} do echo \u0026quot;$Variable\u0026quot; done # =\u0026gt; 1 # =\u0026gt; 2 # =\u0026gt; 3 # Or write it the \u0026quot;traditional for loop\u0026quot; way: for ((a=1; a \u0026lt;= 3; a++)) do echo $a done # =\u0026gt; 1 # =\u0026gt; 2 # =\u0026gt; 3 # They can also be used to act on files.. # This will run the command `cat` on file1 and file2 for Variable in file1 file2 do cat \u0026quot;$Variable\u0026quot; done # ..or the output from a command # This will `cat` the output from `ls`. for Output in $(ls) do cat \u0026quot;$Output\u0026quot; done # Bash can also accept patterns, like this to `cat` # all the Markdown files in current directory for Output in ./*.markdown do cat \u0026quot;$Output\u0026quot; done # while loop: while [ true ] do echo \u0026quot;loop body here...\u0026quot; break done # =\u0026gt; loop body here... # You can also define functions # Definition: function foo () { echo \u0026quot;Arguments work just like script arguments: $@\u0026quot; echo \u0026quot;And: $1 $2...\u0026quot; echo \u0026quot;This is a function\u0026quot; return 0 } # Call the function `foo` with two arguments, arg1 and arg2: foo arg1 arg2 # =\u0026gt; Arguments work just like script arguments: arg1 arg2 # =\u0026gt; And: arg1 arg2... # =\u0026gt; This is a function # or simply bar () { echo \u0026quot;Another way to declare functions!\u0026quot; return 0 } # Call the function `bar` with no arguments: bar # =\u0026gt; Another way to declare functions! # Calling your function foo \u0026quot;My name is\u0026quot; $Name # There are a lot of useful commands you should learn: # prints last 10 lines of file.txt tail -n 10 file.txt # prints first 10 lines of file.txt head -n 10 file.txt # sort file.txt's lines sort file.txt # report or omit repeated lines, with -d it reports them uniq -d file.txt # prints only the first column before the ',' character cut -d ',' -f 1 file.txt # replaces every occurrence of 'okay' with 'great' in file.txt # (regex compatible) sed -i 's/okay/great/g' file.txt #¬†be aware that this -i flag means that file.txt will be changed #¬†-i or --in-place erase the input file (use --in-place=.backup to keep a back-up) # print to stdout all lines of file.txt which match some regex # The example prints lines which begin with \u0026quot;foo\u0026quot; and end in \u0026quot;bar\u0026quot; grep \u0026quot;^foo.*bar$\u0026quot; file.txt # pass the option \u0026quot;-c\u0026quot; to instead print the number of lines matching the regex grep -c \u0026quot;^foo.*bar$\u0026quot; file.txt # Other useful options are: grep -r \u0026quot;^foo.*bar$\u0026quot; someDir/ # recursively `grep` grep -n \u0026quot;^foo.*bar$\u0026quot; file.txt # give line numbers grep -rI \u0026quot;^foo.*bar$\u0026quot; someDir/ # recursively `grep`, but ignore binary files # perform the same initial search, but filter out the lines containing \u0026quot;baz\u0026quot; grep \u0026quot;^foo.*bar$\u0026quot; file.txt | grep -v \u0026quot;baz\u0026quot; # if you literally want to search for the string, # and not the regex, use `fgrep` (or `grep -F`) fgrep \u0026quot;foobar\u0026quot; file.txt # The `trap` command allows you to execute a command whenever your script # receives a signal. Here, `trap` will execute `rm` if it receives any of the # three listed signals. trap \u0026quot;rm $TEMP_FILE; exit\u0026quot; SIGHUP SIGINT SIGTERM # `sudo` is used to perform commands as the superuser # usually it will ask interactively the password of superuser NAME1=$(whoami) NAME2=$(sudo whoami) echo \u0026quot;Was $NAME1, then became more powerful $NAME2\u0026quot; # Read Bash shell built-ins documentation with the bash `help` built-in: help help help help for help return help source help . # Read Bash manpage documentation with `man` apropos bash man 1 bash man bash # Read info documentation with `info` (`?` for help) apropos info | grep '^info.*(' man info info info info 5 info # Read bash info documentation: info bash info bash 'Bash Features' info bash 6 info --apropos bash Source https://learnxinyminutes.com/docs/bash/\n","permalink":"https://elvisfinol.com/posts/bash-in-minutes/","summary":"Bash is a shell program. A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n Note: A binary is a file that contains the instructions for a program, ie it is a ‚Äòprogram‚Äô file, rather than a ‚Äòtext‚Äô file, or an ‚Äòapplication‚Äô file (such as a Word document).","title":"BASH in minutes"},{"content":"Table content  Concept The benefits of a Load Balancer How does it work Type of Load Balancer Load Balancer locations Algorithms  Concept A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers. It helps scale horizontally across an ever-increasing number of servers.\n‚áß back to topThe benefits of a Load Balancer  Reduced the work-load on an individual server. Large amount of work done in same time due to concurrency. Increased performance of your application because of faster response. No single point of failure. In a load balanced environment, if a server crashes the application is still up and served by the other servers in the cluster. When appropriate load balancing algorithm is used, it brings optimal and efficient utilization of the resources, as it eliminates the scenario of some server‚Äôs resources are getting used than others. Scalability: We can increase or decrease the number of servers on the fly without bringing down the application. Load balancing increases the reliability of your enterprise application. Increased security as the physical servers and IPs are abstract in certain cases.  ‚áß back to topHow does it work  Define IP or DNS name for LB: Administrators define one IP address and/or DNS name for a given application, task, or website, to which all requests will come. This IP address or DNS name is the load balancing server. Add backend pool for LB: The administrator will then enter into the load balancing server the IP addresses of all the actual servers that will be sharing the workload for a given application or task. This pool of available servers is only accessible internally, via the load balancer. Deploy LB: Finally, your load balancer needs to be deployed ‚Äî either as a proxy, which sits between your app servers and your users worldwide and accepts all traffic, or as a gateway, which assigns a user to a server once and leaves the interaction alone thereafter. Redirect requests: Once the load balancing system is in place, all requests to the application come to the load balancer and are redirected according to the administrator‚Äôs preferred algorithm.  ‚áß back to topType of Load Balancer  Network Load Balancing: Network load balancing, as its name suggests, leverages network layer information to decide where to send network traffic. This is accomplished through layer 4 load balancing, which is designed to handle all forms of TCP/UDP traffic. Network load balancing is considered the fastest of all the load balancing solutions, but it tends to fall short when it comes to balancing the distribution of traffic across servers. HTTP(S) Load Balancing: HTTP(S) load balancing is one of the oldest forms of load balancing. This form of load balancing relies on layer 7, which means it operates in the application layer. HTTP load balancing is often dubbed the most flexible type of load balancing because it allows you to form distribution decisions based on any information that comes with an HTTP address. Internal Load Balancing: Internal load balancing is nearly identical to network load balancing but can be leveraged to balance internal infrastructure.   When talking about types of load balancers, it‚Äôs also important to note there are hardware load balancers, software load balancers, and virtual load balancers.\n  Hardware Load Balancer: A hardware load balancer, as the name implies, relies on physical, on-premises hardware to distribute application and network traffic. These devices can handle a large volume of traffic but often carry a hefty price tag and are fairly limited in terms of flexibility. Software Load Balancer: A software load balancer comes in two forms‚Äîcommercial or open-source‚Äîand must be installed prior to use. Like cloud-based balancers, these tend to be more affordable than hardware solutions. Virtual Load Balancer: A virtual load balancer differs from software load balancers because it deploys the software of a hardware load balancing device on a virtual machine.  ‚áß back to topLoad Balancer Locations  Between user and web servers (User =\u0026gt; Web Servers) Between web servers and an internal platform layer (application servers, cache servers) (Webservers =\u0026gt; App or Cache servers) Between internal platform layer and database (App or Cache servers =\u0026gt; Database servers)  ‚áß back to topAlgorithms There is a variety of load balancing methods, which use different algorithms best suited for a particular situation.\nRound-robin load balancing is one of the simplest and most used load balancing algorithms. Client requests are distributed to application servers in rotation. For example, if you have three application servers: the first client request to the first application server in the list, the second client request to the second application server, the third client request to the third application server, the fourth to the first application server and so on. This load balancing algorithm does not take into consideration the characteristics of the application servers i.e. it assumes that all application servers are the same with the same availability, computing and load handling characteristics.\nWeighted Round Robin builds on the simple Round-robin load balancing algorithm to account for differing application server characteristics. The administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability.\nIf application server #1 is twice as powerful as application server #2 (and application server #3), application server #1 is provisioned with a higher weight and application server #2 and #3 get the same weight. If there five (5) sequential client requests, the first two (2) go to application server #1, the third (3) goes to application server #2, the fourth (4) to application server #3 and the fifth (5) to application server #1.\nLeast Connection load balancing is a dynamic load balancing algorithm where client requests are distributed to the application server with the least number of active connections at the time the client request is received. In cases where application servers have similar specifications, an application server may be overloaded due to longer lived connections; this algorithm takes the active connection load into consideration.\nWeighted Least Connection builds on the Least Connection load balancing algorithm to account for differing application server characteristics. The administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability. The LoadMaster is making the load balancing criteria based on active connections and application server weighting.\nResource Based (Adaptive) is a load balancing algorithm requires an agent to be installed on the application server that reports on its current load to the load balancer. The installed agent monitors the application servers availability status and resources. The load balancer queries the output from the agent to aid in load balancing decisions.\nSDN Adaptive is a load balancing algorithm that combines knowledge from Layers 2, 3, 4 and 7 and input from an SDN Controller to make more optimized traffic distribution decisions. This allows information about the status of the servers, the status of the applications running on them, the health of the network infrastructure, and the level of congestion on the network to all play a part in the load balancing decision making.\nFixed Weighting is a load balancing algorithm where the administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability. The application server with the highest weigh will receive all of the traffic. If the application server with the highest weight fails, all traffic will be directed to the next highest weight application server.\nWeighted Response Time is a load balancing algorithm where the response times of the application servers determines which application server receives the next request. The application server response time to a health check is used to calculate the application server weights. The application server that is responding the fastest receives the next request.\nSource IP hash load balancing algorithm that combines source and destination IP addresses of the client and server to generate a unique hash key. The key is used to allocate the client to a particular server. As the key can be regenerated if the session is broken, the client request is directed to the same server it was using previously. This is useful if it‚Äôs important that a client should connect to a session that is still active after a disconnection.\nURL Hash is a load balancing algorithm to distribute writes evenly across multiple sites and sends all reads to the site owning the object.\n‚áß back to topReferences  https://www.thegeekstuff.com/2016/01/load-balancer-intro/ https://avinetworks.com/glossary/server-load-balancer/ https://medium.com/must-know-computer-science/system-design-load-balancing-1c2e7675fc27 https://lumecloud.com/what-does-a-load-balancer-do/  ","permalink":"https://elvisfinol.com/posts/load-balancing-deep-dive/","summary":"Table content  Concept The benefits of a Load Balancer How does it work Type of Load Balancer Load Balancer locations Algorithms  Concept A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers. It helps scale horizontally across an ever-increasing number of servers.\n‚áß back to topThe benefits of a Load Balancer  Reduced the work-load on an individual server.","title":"Load Balancing [Deep Dive]"},{"content":"The following information can help you troubleshoot issues with your Application Load Balancer.\nSuccessful request: Code 200 ‚úÖ Unsuccessfull at client site: 4XX code ‚ùå  Error 400: Bad Request Error 401: Unauthorized Error 403: Forbidden Error 405: Method not allowed Error 408: Request timeout Error 413: Payload too large Error 414: URI too long Error 460: Client close connection Error 463: X-Forwarded For header with \u0026gt; 30 IP (Similar to malformed request)  Unsuccessfull at server site: 5XX code ‚ùå  Error 500: Internal server error would mean some error on the ELB itseft Error 501: Not Implemented Error 502 : Bad Gateway Error 503: Service Unavailable Error 504: Gateway timeout, probably an issue within the server Error 505: Version not supported Error 561: Unauthorized  references https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\n","permalink":"https://elvisfinol.com/posts/aws-loadbalancer-error-codes/","summary":"The following information can help you troubleshoot issues with your Application Load Balancer.\nSuccessful request: Code 200 ‚úÖ Unsuccessfull at client site: 4XX code ‚ùå  Error 400: Bad Request Error 401: Unauthorized Error 403: Forbidden Error 405: Method not allowed Error 408: Request timeout Error 413: Payload too large Error 414: URI too long Error 460: Client close connection Error 463: X-Forwarded For header with \u0026gt; 30 IP (Similar to malformed request)  Unsuccessfull at server site: 5XX code ‚ùå  Error 500: Internal server error would mean some error on the ELB itseft Error 501: Not Implemented Error 502 : Bad Gateway Error 503: Service Unavailable Error 504: Gateway timeout, probably an issue within the server Error 505: Version not supported Error 561: Unauthorized  references https://docs.","title":"[CHEAT SHEET üìí] - AWS Application Load Balancer Error Codes"},{"content":"Here you will find several methods to access your EC2 if you lost you SSH key\nIf the instance is EBS backed:  Stop the instance, detach the root volume Attach the root volume to another instance as a data volume Modify the ~/.ssh/authorized_keys file with your new key Move the the volume back to the stopped instance Start the instance and you can SSH into it again  If the instance is EBS (new method üßô):  Run the AWSSupport-ResetAccess automation document in SSM  Instance Store backed EC2:  You cannot stop the instance (otherwise data is lost) - AWS recommends termination Use Session Manager access and edit the ~/.ssh/authorized_keys file directly  references  Connect to your Linux instance if you lose your private key. AWSSupport-ResetAccess  ","permalink":"https://elvisfinol.com/posts/aws-tutorial-method-to-recover-ssh-key/","summary":"Here you will find several methods to access your EC2 if you lost you SSH key\nIf the instance is EBS backed:  Stop the instance, detach the root volume Attach the root volume to another instance as a data volume Modify the ~/.ssh/authorized_keys file with your new key Move the the volume back to the stopped instance Start the instance and you can SSH into it again  If the instance is EBS (new method üßô):  Run the AWSSupport-ResetAccess automation document in SSM  Instance Store backed EC2:  You cannot stop the instance (otherwise data is lost) - AWS recommends termination Use Session Manager access and edit the ~/.","title":"[How-to] I lost my SSH key for EC2! Steps to get into your instance back"},{"content":"How does it works AWS System Manager?\n AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and enables you to automate operational tasks across your AWS resources.\n Table content  Overview Features Documents Syntax Run Command Patch Manager Inventory State Manager Agent  Overview  Helps you manage your EC2 and On-Premise systems at scale. Get operation insights about the state of you infraestructure. Easily detect problems. Patching automation for enhanced compliance. Works for both Windows and Linux OS. Integrated with CloudWatch metrics / dashboards. Integrated with AWS Config.  ‚áß back to topFeatures  Resource Groups  Create, view or manage logical group of resources thanks to tags. Allow creation of logical groups of resources such as:  Applications. Different layers of an applicaton stack. Production versus development environments.   Regional service (you need to create diffente resource groups if you are operating in differente regions). Works with EC2, S3, DynamoDB, Lambda and many more.   Insights:  Insights Dashboard. Invertory: discovery and audit the software installed. Compliance.   Parameter Store Action:  Automation (shut down EC2, create AMIs). Run Command. Session Manager. Patch Manager. Maintenance Windows. State Manager: define and maintaining configuration of OS and applications.    ‚áß back to topDocuments Syntax  You define parameters and actions. Can be apply to State Manager, Patch Manager, Automation, Run Command and Parameter Store. Can be YAML or JSON:  DirectoryType: type: String description: \u0026#34;(Required) The directory type to launch.\u0026#34; default: AwsMad allowedValues: - AdConnector - AwsMad - SimpleAd \u0026#34;DirectoryType\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(Required) The directory type to launch.\u0026#34;, \u0026#34;default\u0026#34;: \u0026#34;AwsMad\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;AdConnector\u0026#34;, \u0026#34;AwsMad\u0026#34;, \u0026#34;SimpleAd\u0026#34; ] } ‚áß back to topRun Command  helps perform on-demand changes like updating applications or running Linux shell scripts and Windows PowerShell commands on a target set of dozens or hundreds of instances. Execute a document (script) or just run a command Run command across multiple instances (using resource groups) Rate Control / Error Control Integrated with IAM \u0026amp; CloudTrial No need for SSH Results in the console  ‚áß back to topPatch Manager  Inventory =\u0026gt; List Software on an instance Inventory + Run Command =\u0026gt; Patch Software Patch Manager + Maintenance Window =\u0026gt; Patch OS helps automate process of patching managed instances with both security related and other types of updates. helps apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications.) enables scanning of instances for missing patches and applies them individually or to large groups of instances by using EC2 instance tags. uses patch baselines, which can include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. helps install security patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.  ‚áß back to topInventory  provides visibility into your Amazon EC2 and on-premises computing environment collect metadata from the managed instances about applications, files, components, patches, and more on your managed instances  ‚áß back to topState Manager  helps automate the process of keeping the managed instances in a defined state. helps ensure that the instances are bootstrapped with specific software at startup, joined to a Windows domain (Windows instances only), or patched with specific software updates.  ‚áß back to topAgent  is software that can be installed and configured on an EC2 instance, an on-premises server, or a virtual machine (VM). makes it possible for Systems Manager to update, manage, and configure these resources. must be installed on each instance to use with Systems Manager. usually comes preinstalled with lot of Amazon Machine Images (AMIs), while it must be installed manually on other AMIs, and on on-premises servers and virtual machines for your hybrid environment.  ‚áß back to top","permalink":"https://elvisfinol.com/posts/aws-system-manager/","summary":"How does it works AWS System Manager?\n AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and enables you to automate operational tasks across your AWS resources.\n Table content  Overview Features Documents Syntax Run Command Patch Manager Inventory State Manager Agent  Overview  Helps you manage your EC2 and On-Premise systems at scale.","title":"Managing EC2 Instances at scale - System Manager (SSM)"},{"content":"Before start please install Apache HTTP Server on the instance \u0026gt; following the ‚Äúyum install httpd‚Äù command and create a simple index.html file under path /var/www/html/index.html\nAfter that, start httpd agent an validate accessing over the public DNS, you should see your ‚ÄúHello World‚Äù. If you see that on your browser this means that your server is running properly.\nRemember we will catch the following logs. So we need to put both path on the CloudWatch wizard later.\n/var/log/httpd/access_log \u0026amp; /var/log/httpd/error_log\nInstalling the CloudWatch Agent On this step we install the Unified CloudWatch Agent that will allows us to send metrics and logs into CloudWatch. You can store and retrieve configuration into the SSM Parameter Store and allow you to have quick setup for all your instances if you want to have them all configure the same way!\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm\nsudo rpm -U ./amazon-cloudwatch-agent.rpm\nI had already install\nRun the wizard\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard ============================================================= = Welcome to the AWS CloudWatch Agent Configuration Manager = ============================================================= On which OS are you planning to use the agent? 1. linux 2. windows 3. darwin default choice: [1]: 1 Trying to fetch the default region based on ec2 metadata... Are you using EC2 or On-Premises hosts? 1. EC2 2. On-Premises default choice: [1]: 1 Which user are you planning to run the agent? 1. root 2. cwagent 3. others default choice: [1]: 1 Do you want to turn on StatsD daemon? 1. yes 2. no default choice: [1]: 1 Which port do you want StatsD daemon to listen to? default choice: [8125] What is the collect interval for StatsD daemon? 1. 10s 2. 30s 3. 60s default choice: [1]: 1 What is the aggregation interval for metrics collected by StatsD daemon? 1. Do not aggregate 2. 10s 3. 30s 4. 60s default choice: [4]: 6 The value 6 is not valid to this question. Please retry to answer: What is the aggregation interval for metrics collected by StatsD daemon? 1. Do not aggregate 2. 10s 3. 30s 4. 60s default choice: [4]: 4 Do you want to monitor metrics from CollectD? 1. yes 2. no default choice: [1]: 1 Do you want to monitor any host metrics? e.g. CPU, memory, etc. 1. yes 2. no default choice: [1]: 1 Do you want to monitor cpu metrics per core? Additional CloudWatch charges may apply. 1. yes 2. no default choice: [1]: 1 Do you want to add ec2 dimensions (ImageId, InstanceId, InstanceType, AutoScalingGroupName) into all of your metrics if the info is available? 1. yes 2. no default choice: [1]: 1 Would you like to collect your metrics at high resolution (sub-minute resolution)? This enables sub-minute resolution for all metrics, but you can customize for specific metrics in the output json file. 1. 1s 2. 10s 3. 30s 4. 60s default choice: [4]: 4 Which default metrics config do you want? 1. Basic 2. Standard 3. Advanced 4. None default choice: [1]: 1 Current config as follows: { \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;collectd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60 }, \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 }, \u0026quot;statsd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60, \u0026quot;metrics_collection_interval\u0026quot;: 10, \u0026quot;service_address\u0026quot;: \u0026quot;:8125\u0026quot; } } } } Are you satisfied with the above config? Note: it can be manually customized after the wizard completes to add additional items. 1. yes 2. no default choice: [1]: 1 Do you have any existing CloudWatch Log Agent (http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html) configuration file to import for migration? 1. yes 2. no default choice: [2]: 2 Do you want to monitor any log files? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/access_log Log group name: default choice: [access_log] Log stream name: default choice: [{instance_id}] Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/error_log Log group name: default choice: [error_log] Log stream name: default choice: [{instance_id}] Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 2 Saved config file to /opt/aws/amazon-cloudwatch-agent/bin/config.json successfully. Current config as follows: { \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;logs\u0026quot;: { \u0026quot;logs_collected\u0026quot;: { \u0026quot;files\u0026quot;: { \u0026quot;collect_list\u0026quot;: [ { \u0026quot;file_path\u0026quot;: \u0026quot;/var/log/httpd/access_log\u0026quot;, \u0026quot;log_group_name\u0026quot;: \u0026quot;access_log\u0026quot;, \u0026quot;log_stream_name\u0026quot;: \u0026quot;{instance_id}\u0026quot; }, { \u0026quot;file_path\u0026quot;: \u0026quot;/var/log/httpd/error_log\u0026quot;, \u0026quot;log_group_name\u0026quot;: \u0026quot;error_log\u0026quot;, \u0026quot;log_stream_name\u0026quot;: \u0026quot;{instance_id}\u0026quot; } ] } } }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;collectd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60 }, \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 }, \u0026quot;statsd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60, \u0026quot;metrics_collection_interval\u0026quot;: 10, \u0026quot;service_address\u0026quot;: \u0026quot;:8125\u0026quot; } } } } Please check the above content of the config. The config file is also located at /opt/aws/amazon-cloudwatch-agent/bin/config.json. Edit it manually if needed. Do you want to store the config in the SSM parameter store? 1. yes 2. no default choice: [1]: 1 What parameter store name do you want to use to store your config? (Use 'AmazonCloudWatch-' prefix if you use our managed AWS policy) default choice: [AmazonCloudWatch-linux] Trying to fetch the default region based on ec2 metadata... Which region do you want to store the config in the parameter store? default choice: [us-east-1] Which AWS credential should be used to send json config to parameter store? 1. ASIA4ZKOFOINW3ZORZGS(From SDK) 2. Other default choice: [1]: Please make sure the creds you used have the right permissions configured for SSM access. Which AWS credential should be used to send json config to parameter store? 1. ASIA4ZKOFOINW3ZORZGS(From SDK) 2. Other default choice: [1]: Successfully put config to parameter store AmazonCloudWatch-linux. Program exits now.  You can see JSON config is already on the Parameter Store. So any other EC2 instances will bootup and fetch the value of this config.\nIn order to point to the SSM Parameter you have two options:\nFetch the config\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux  Reading directly JSON file\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: us-east-1 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2021/01/22 15:04:52 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase failed ======== Error Log ======== 2021-01-22T15:04:52Z E! [telegraf] Error running agent: Error parsing /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml, open /usr/share/collectd/types.db: no such file or directory [root@ip-172-31-55-22 ec2-user]#  We successfully fetch the config. Probably you will see an error that /usr/share/collectd/types.db is missing.\nTo solve this create the folders/file and re-run the agent\n[root@ip-172-31-55-22 ec2-user]# mkdir -p /usr/share/collectd/ [root@ip-172-31-55-22 ec2-user]# touch /usr/share/collectd/types.db [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: us-east-1 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2021/01/22 15:10:13 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase succeeded Configuration validation succeeded amazon-cloudwatch-agent has already been stopped Created symlink from /etc/systemd/system/multi-user.target.wants/amazon-cloudwatch-agent.service to /etc/systemd/system/amazon-cloudwatch-agent.service. Redirecting to /bin/systemctl restart amazon-cloudwatch-agent.service [root@ip-172-31-55-22 ec2-user]#  Now agent is WORKING and you should start seeing CloudWatchLogs and Metrics! üí™ Go to CloudWatch and check it out:\nLogs\nMetrics\nreferences\n Installing and Running the CloudWatch Agent on Your Servers  ","permalink":"https://elvisfinol.com/posts/how-to-install-cloudwatch-agent-on-ec2/","summary":"Before start please install Apache HTTP Server on the instance \u0026gt; following the ‚Äúyum install httpd‚Äù command and create a simple index.html file under path /var/www/html/index.html\nAfter that, start httpd agent an validate accessing over the public DNS, you should see your ‚ÄúHello World‚Äù. If you see that on your browser this means that your server is running properly.\nRemember we will catch the following logs. So we need to put both path on the CloudWatch wizard later.","title":"Tutorial - How to install CloudWatch Agent on EC2 Instances?"},{"content":"Hey there! Here you will find this cheat sheet which I prepared to show, all linux commands which I use to day a day, in my actual role as System Engineer.\nTable content  Help Users Network Files-Folders Remote  More coming soon! Stay tuned\u0026hellip;\n Permissions Editors Differences Packages Compressor   Help Help commands\n man manual page (wide) whatis short description apropos related help  ‚áß back to topUsersüë§ Change and info\n whoami current user su switch to user [root] visudo edit sudoers [vi+sudo]  System users\n useradd create/update user  adduser friendly add user   userdel delete user  deluser friendly del user   usermod modify user account groups group members passwd change password Islogins show known users  Logged users\n who current logged users  w current logged users \u0026amp; data   users current logged users last last logged users \u0026amp; reboots lastb last bad logins lastlog recent login users  ‚áß back to topNetworküåê Net configuration\n ifconfig config ip/net features  ip new ifconfig tool   dhclient DHCP client  DNS and domains\n ping send ICMP to hosts nslookup query DNS lookup  dig DNS lookup utility   whois whois domain name or ip  Trace route\n traceroute print route packets tracepath trace path  mtr network diagnostic tool    Network tools\n nmap network security scanner nc cat via network connection ss show sockets statistics  Network monitoring\n bmon bandwidth monitor iftop interface network monitor nethogs net monitor by process wondershaper bandwidth limit Iptraf-ng network monitor tcpdump network activity dump netstat print network statistics  Mac address\n arp show mac/ip address cache arping ping mac address  Firewall\n  iptables ip packet filter \u0026amp; NAT\n shorewall firewall for iptables ufw firewall for iptables  ‚áß back to top  Files-Folders Folders (Directories)\n mkdir create dir  -p all dirs   pwd current dir ls list files \u0026amp; dirs  -l long data -h human -R recursive   cd change to dir pushd / popd directory stack autojump smart jump to dirs tree list files in tree format  File handling\n file show file type touch update/create mv move/rename files  -f force -u update   cp copy files  -f force -r recursive   rm remove files ln make link to file stat filesytem stats  Fine files\n  type display command type\n  Find search files in a dir\n locate search files in database updatedb update file database    whereis locate binary/manpage\n  which get binary file pathname\n‚áß back to top  Remote  telnet telnet connection ftp file transfer connect  ssh remote connection   sftp connect ftp via ssh sshfs connect disk via ssh  SCP/SSH SYNTAX\nscp user@ip:/folder remote ‚áß back to topMore coming soon! Stay tuned\u0026hellip;\n","permalink":"https://elvisfinol.com/posts/my-linux-terminal-cheatsheet/","summary":"Hey there! Here you will find this cheat sheet which I prepared to show, all linux commands which I use to day a day, in my actual role as System Engineer.\nTable content  Help Users Network Files-Folders Remote  More coming soon! Stay tuned\u0026hellip;\n Permissions Editors Differences Packages Compressor   Help Help commands\n man manual page (wide) whatis short description apropos related help  ‚áß back to topUsersüë§ Change and info","title":"[CHEAT SHEET üìí] - My Linux Terminal"},{"content":"You have a theme you added as a git submodule and you recently re-cloned your project. In order to fix this, your submodule needs to be re-downloaded as well.\nRun the following commands:\ngit submodule init   git submodule update   hugo server   Then your project will load without errors.\n","permalink":"https://elvisfinol.com/posts/how-to-fix-layout-no-found-in-hugo/","summary":"You have a theme you added as a git submodule and you recently re-cloned your project. In order to fix this, your submodule needs to be re-downloaded as well.\nRun the following commands:\ngit submodule init   git submodule update   hugo server   Then your project will load without errors.","title":"[Solved] How to fix the error found no layout file for HTML for page in Hugo?"},{"content":"Al utilizar el protocolo SSH, puedes conectar y autenticar a servidores remotos y servicios. La ventaja para el caso de GitHub es que no necesitas suministrar ‚Äúusername‚Äù y ‚Äúpersonal token‚Äù en cada deploy.\nComandos Paso 1: Generar llave SSH\nAseg√∫rate que Git Bash esta abierto. Para generar una llave SSH ejecuta este comando:\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;ejemplo@ejemplo.com\u0026quot;\n(NO olvides reemplazar el email ‚Äúejemplo@ejemplo.com‚Äù por tu email real)\nPaso 2: Uso de llave ** Ahora que la llave esta generada, vamos a utilizarla! Primero debemos iniciar el agente de SSH ejecutando:\n$ eval $(ssh-agent -s)\nUna vez iniciado el agente, vamos a agregar la llave que hemos generado. Ten en cuenta que si seleccionas un path diferente al default, debes cambiarlo en el comando:\n$ ssh-add ~/.ssh/id_rsa\n****Paso 3: Agregar llave SSH en GitHub\nAhora que tenemos la llave ssh configurada en nuestra PC, procedemos agregarla en GitHub. Para ello puedes ejecutar este comando para copiarla y luego pegarla en GitHub.\n$ clip \u0026lt; ~/.ssh/id_rsa.pub\nVe a las Settings en tu GitHub, en el sidebar haz click en ‚ÄúSSH and GPG Keys‚Äù\nHaz click en ‚ÄúNew SSH Key‚Äù\nEn title puedes colocar la descripci√≥n, por ejemplo ‚ÄúMi computador personal‚Äù y seguidamente vas a pegar tu llave en ‚ÄúKey‚Äù.\nLuego haces click en ‚ÄúAdd SSH key‚Äù, te pedir√° las credenciales de tu cuenta en GitHub. Al llegar a este punto tu llave ha sido agregada y puedes empezar a hacer push desde tu repositorio remoto a GitHub. üôÇ\n","permalink":"https://elvisfinol.com/posts/configurando-ssh-con-github-windows/","summary":"Al utilizar el protocolo SSH, puedes conectar y autenticar a servidores remotos y servicios. La ventaja para el caso de GitHub es que no necesitas suministrar ‚Äúusername‚Äù y ‚Äúpersonal token‚Äù en cada deploy.\nComandos Paso 1: Generar llave SSH\nAseg√∫rate que Git Bash esta abierto. Para generar una llave SSH ejecuta este comando:\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;ejemplo@ejemplo.com\u0026quot;\n(NO olvides reemplazar el email ‚Äúejemplo@ejemplo.com‚Äù por tu email real)","title":"Tutorial - Configurando SSH con GitHub (Windows)"},{"content":"Frameworks, guides and several tools to help you improve your writing CSS.\nTable content  Frameworks Tools Preprocessors Reset and Normalize Design Inspiration Illustrations Placeholder Games Icons  Frameworksüé® CSS framework gives web developers a basic structure, which includes grid, interactive UI patterns, web typography, tooltips, buttons, form elements, icons. This structure helps web developers to start quickly and efficiently when they are designing a website or web applications.\n awsm.css - Simple CSS library for semantic HTML markup. Bonsai - A complete Utility First CSS Framework for less than 50kb. Bootstrap - The most popular HTML, CSS, and JS framework. Bulma - A modern CSS framework based on Flexbox. Also has Sass import for modification. Butter Cake - A Modern Lightweight Front End CSS framework for faster and easier web development. Charts.css - CSS data visualization framework. Chota - A responsive, customizable micro-framework (3kb) with helpful utilities and a grid system. Cirrus - A fully responsive and comprehensive CSS framework with beautiful controls and simplistic structure. eFrolic - CSS framework which without using JavaScript is interactive and animated. Foundation - advanced responsive front-end framework. Gralig - A modest, grayish CSS library. Halfmoon - A responsive front-end framework with a built-in dark mode. Hasser CSS - A lightweight (12k, not minified) but useful CSS framework with flexible Grid, Hero and more. inuit.css - Powerful, scalable, Sass-based, BEM, OOCSS framework. material-components-web - Modular and customizable Material Design UI components for the web. Materialize - A modern responsive front-end framework based on Material Design. Milligram - A minimalist CSS framework. Numl - An HTML-based language and design system that lets you create responsive and accessible high-quality web interfaces with any look. Pure.css - A set of small, responsive CSS modules that you can use in every web project. Semantic UI - Powerful framework that uses human-friendly HTML. Shorthand Framework - Feature rich CSS framework for the new decade. Spectre.css - A lightweight, responsive and modern CSS framework. Strawberry - A set of common flexbox utilities focused on making your life easier and faster with nested flexboxes. Tachyons - Functional CSS for humans. Tacit - CSS framework for dummies with zero skills in graphic design. tailwindcss - A utility-first CSS framework for rapid UI development. UIkit - A lightweight and modular front-end framework. unsemantic - Fluid grid for mobile, tablet, and desktop. Wing - A Minimal, Lightweight, Responsive framework.  ‚áß back to topToolsüîß  Codepen - The best place to build, test, and discover front-end code. Pleese Play - All the annoying CSS stuff we don\u0026rsquo;t want to do in 1 tool. CCSmatic - The ultimate CSS tools for web designers.  ‚áß back to topPreprocessors‚öôÔ∏è A CSS preprocessor is a program that lets you generate CSS from the preprocessor\u0026rsquo;s own unique syntax. There are many CSS preprocessors to choose from, however most CSS preprocessors will add some features that don\u0026rsquo;t exist in pure CSS, such as mixin, nesting selector, inheritance selector, and so on. These features make the CSS structure more readable and easier to maintain.\n LESS - Backwards compatible with CSS, and the extra features it adds use existing CSS syntax. PostCSS - Transforming CSS with JS plugins. Sass - Mature, stable, and powerful professional-grade CSS extension language. STYLIS - Light-weight CSS preprocessor. Stylus - Expressive, robust, feature-rich CSS language built for NodeJs.  ‚áß back to topReset and Normalize  Normalize - A set of CSS rules that provide better cross-browser consistency in the default styling of HTML elements. Normalize-OpenType - Adds OpenType features‚Äîligatures, kerning, and more‚Äîto Normalize.css. MiniReset.css - A tiny modern CSS reset. sanitize.css - A set of CSS rules that style with today‚Äôs best practices out-of-the-box. unstyle.css - Specialised stylesheet for removing user agent styles, style the web with your baseline. reset.css - CSS Tools: Reset CSS.  ‚áß back to topDesign Inspiration  Behance - is a social media platform owned by Adobe \u0026ldquo;to showcase and discover creative work\u0026rdquo; Dribbble - is a self-promotion and social networking platform for digital designers and creatives. Land-book - design gallery with the best and most carefully collected websites. We help creatives find inspiration \u0026amp; motivation to do rad stuff. awwwards - are the Website Awards that recognize and promote the talent and effort of the best developers, designers and web agencies in the world.  ‚áß back to topIllustrations  Pexels - The best free stock photos \u0026amp; videos shared by talented creators. unDraw - Open-source illustrations for any idea you can imagine and create. DrawKit - Hand-drawn vector illustration and icon resources, perfect for your next project. freepik - Graphic resources for everyone. pixabay - Stunning free images \u0026amp; royalty free stock. icons8 - Free vector illustrations to class up your project.  ‚áß back to topPlaceholder  Placedog - Hand-picked, stunning photographs of our favourite pets: dogs! Free to use with your websites/projects. Placekitten - A quick and simple service for getting pictures of kittens for use as placeholders in your designs or code. Placeholder - is a free image placeholder service for web designers, serving billions and billions of images each year. Picsum - The Lorem Ipsum for photos.  GamesüïπÔ∏è  Grid Garden - A game where you write CSS code to grow your carrot garden. ü•ï Flexbox Froggy - A game where you help Froggy and friends by writing CSS code. üê∏ Flexbox Defense - Tower Defense with a twist: all towers must be positioned with CSS Flexbox. CSS Dinner - It\u0026rsquo;s a fun game to learn and practice CSS selectors.  ‚áß back to topIcons  Font Awesome - Get vector icons and social logos on your website with Font Awesome, the web\u0026rsquo;s most popular icon set and toolkit. Ikonate - Ikonate are fully customisable \u0026amp; accessible*, well-optimised vector icons. LineIcons - Handcrafted Line Icons for Modern User Interfaces of Web, Android, iOS, and Desktop App Projects. TheNounProject - Icons and Photos For Everything. MaterialDesignIcons - Beautifully crafted symbols for common actions and items. FlatIcon - The best UI icons for your projects. IconFinder - is the world\u0026rsquo;s largest marketplace for vector and raster icons in SVG and PNG formats. FeatherIcons - is a collection of simply beautiful open source icons. Each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency and readability. Favicon Generator - Favicons are also known as favorite icons, URL icons, or website icon. They are very small icons (usually 16√ó16 or 32√ó32) and appear next to your website URL in browser tabs and bookmarks, making it easy for your visitors to identify your website in their open tabs. StreamLineIcons - The world‚Äôs largest icon pack. LinearIcons - is the highest quality set of line icons, matching with minimalist UI designs in iOS. Unicons - Web‚Äôs new favorite icon library. NucleoApp - Nucleo is a beautiful library of 30635 icons, and a powerful application to collect, customize and export all your icons. CoreUI Icons - Premium designed free icon set with marks in SVG, Webfont and raster formats. Line-Awesome - Swap Font Awesome for modern line icons in one line of code.  ‚áß back to top","permalink":"https://elvisfinol.com/posts/css-tools/","summary":"Frameworks, guides and several tools to help you improve your writing CSS.\nTable content  Frameworks Tools Preprocessors Reset and Normalize Design Inspiration Illustrations Placeholder Games Icons  Frameworksüé® CSS framework gives web developers a basic structure, which includes grid, interactive UI patterns, web typography, tooltips, buttons, form elements, icons. This structure helps web developers to start quickly and efficiently when they are designing a website or web applications.\n awsm.","title":"CSS Tools \u0026 Resourcesüåà"},{"content":"¬°Hola! Mi nombre es Elvis Finol. Nac√≠ hace 30 a√±os en Maracaibo (Venezuela). Estudi√© Ingenier√≠a en Electr√≥nica Menci√≥n Automatizaci√≥n y Control de Procesos en la Universidad Rafael Belloso Chac√≠n. En el 2015 decid√≠ salir de mi pa√≠s en busca de mejores oportunidades tanto para el crecimiento personal como profesional. Me mud√© a la capital Argentina, la ic√≥nica Ciudad de Buenos Aires.\n Marzo de 2021 \u0026ndash;\u0026gt; nueva localizaci√≥n / Madrid, Espa√±a. (Historia en desarrollo\u0026hellip;‚úçÔ∏è)\n A pensar que mi t√≠tulo diga lo contrario, desde siempre me he desarrollado en al √°rea de Sistemas. He extendido mis conocimientos en una amplia variedad de entornos (Infraestructura on Premise y Cloud). D√≠a a d√≠a me enfrento a la resoluci√≥n de problemas y a mantener tanto sistemas Productivos como DR; participando en proyectos que van desde la instalaci√≥n de infraestructura, el desarrollo, las pruebas, hasta implementaci√≥n y liberaci√≥n de procesos productivos. Esto me ha llevado a generar una gran habilidad de trabajo bajo presi√≥n.\n Experiencia en Datacenter Infrastructure, Networking, Public Cloud (AWS), Data Warehousing (Teradata), Project Management.\n Alguno de los proyectos recientes que he participado puedo mencionar los siguientes:\nTeradata\n Banco Central De La Rep√∫blica Argentina ‚Äì Floor sweep y migraci√≥n a Teradata 16.20 incluyendo la implementaci√≥n de nueva soluci√≥n BAR/Librer√≠a (2020) HSBC - Instalaci√≥n/migraci√≥n de nueva plataforma Intellibase DEVUAT (2019) Prisma Medios De Pagos ‚Äì Gerente de proyecto para Customer Services en Floor sweep y migraci√≥n a Teradata 16.20 Telef√≥nica Argentina ‚Äì Floor sweep y migraci√≥n a Teradata 16.20, trabajando como Gerente de proyecto para Customer Services. (2019) Banco Galicia ‚Äì L√≠der para migraci√≥n de toda la infraestructura de Backup y Restore, incluyendo implementaci√≥n UDA (Unified Data architecture) (2018) Carrefour (retail) ‚Äì L√≠der para expansi√≥n de Teradata (Merge) llevando sistema 6700 2+1 a 6800 1+1 (2017)  Thomson Reuters\n Project: \u0026ldquo;Argentina Markets Guide\u0026rdquo; desarrollada para Thomson Reuters Eikon (App Financiera)   Este √∫ltimo a√±o me he estado formando, aprendiendo tecnolog√≠as Front-end para el Desarrollo Web (HTML, CSS y JavaScript entre otras) y he realizado algunos projectos que actualmente tengo disponibles en mi GitHub.\n Un poco mas de mi - Hobbies! üí• M√∫sica Desde muy chico descubr√≠ que me encantaba la m√∫sica. Recuerdo las visitas a las tiendas de discos donde guardaba los pocos ahorros que ten√≠a para comprar los √∫ltimos √°lbumes (aquellos tiempos). No fue hasta los 12 a√±os que mi pap√° me regalo una guitarra espa√±ola y ah√≠ no pare de aprender y hacer m√∫sica. Aprend√≠ a tocar el bajo el√©ctrico, guitarra, teclado y lo que m√°s disfruto es la producci√≥n; estar detr√°s de backstage, en mi espacio creando m√∫sica. Te dejo este enlace de mis producciones en el √°rea de la m√∫sica.\nContenido Visual y Fotograf√≠a En el 2019, Mica y yo decidimos crear Vive Simple un espacio en Youtube e Instagram en el que ambos pudi√©ramos canalizar nuestra creatividad, compartiendo nuestras experiencias. Todo esto con el fin de ayudar a las personas y darles a conocer lo hermoso que es Buenos Aires, Argentina.\n¬°Escr√≠beme! üì• No dudes en contactarme si consideras que puedo ser de valor en tu negocio. ¬°Nos vemos y gracias por visitar este espacio!üëã\n","permalink":"https://elvisfinol.com/aboutme/","summary":"sobremi","title":"About Me"},{"content":"","permalink":"https://elvisfinol.com/archive/","summary":"archives","title":"Blog"},{"content":"","permalink":"https://elvisfinol.com/search/","summary":"search","title":"Search"}]