[{"content":"SEARCHING ALGORITHMS BINARY SEARCH binarySearch(input, searchValue) { lower = 0 upper = input.len - 1 white(upper \u0026gt;= lower) { mid = (upper + lower) / 2 if (input[mid] == searchValue) return True elif (searchValue \u0026lt; input[mid]) upper = mid - 1 else: low = mid + 1 } return false } LINEAR SEARCH linearSearch(input, searchValue) { for (i=0 to input.lenght - 1) { if (input[i] == searchValue) return true } return false } JUMP SEARCH # Python3 code to implement Jump Search import math def jumpSearch( arr , x , n ): # Finding block size to be jumped step = math.sqrt(n) # Finding the block where element is # present (if it is present) prev = 0 while arr[int(min(step, n)-1)] \u0026lt; x: prev = step step += math.sqrt(n) if prev \u0026gt;= n: return -1 # Doing a linear search for x in # block beginning with prev. while arr[int(prev)] \u0026lt; x: prev += 1 # If we reached next block or end # of array, element is not present. if prev == min(step, n): return -1 # If element is found if arr[int(prev)] == x: return prev return -1 # Driver code to test function arr = [ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610 ] x = 55 n = len(arr) # Find the index of 'x' using Jump Search index = jumpSearch(arr, x, n) # Print the index where 'x' is located print(\u0026quot;Number\u0026quot; , x, \u0026quot;is at index\u0026quot; ,\u0026quot;%.0f\u0026quot;%index) SORTING ALGORITHMS BUBBLE SORT # Python program for implementation of Bubble Sort def bubbleSort(arr): n = len(arr) # Traverse through all array elements for i in range(n-1): # range(n) also work but outer loop will repeat one time more than needed. # Last i elements are already in place for j in range(0, n-i-1): # traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if arr[j] \u0026gt; arr[j+1] : arr[j], arr[j+1] = arr[j+1], arr[j] # Driver code to test above arr = [64, 34, 25, 12, 22, 11, 90] bubbleSort(arr) print (\u0026quot;Sorted array is:\u0026quot;) for i in range(len(arr)): print (\u0026quot;%d\u0026quot; %arr[i]), INSERTION SORT # Python program for implementation of Insertion Sort # Function to do insertion sort def insertionSort(arr): # Traverse through 1 to len(arr) for i in range(1, len(arr)): key = arr[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j \u0026gt;=0 and key \u0026lt; arr[j] : arr[j+1] = arr[j] j -= 1 arr[j+1] = key # Driver code to test above arr = [12, 11, 13, 5, 6] insertionSort(arr) print (\u0026quot;Sorted array is:\u0026quot;) for i in range(len(arr)): print (\u0026quot;%d\u0026quot; %arr[i]) SELECTION SORT # Python program for implementation of Selection # Sort import sys A = [64, 25, 12, 22, 11] # Traverse through all array elements for i in range(len(A)): # Find the minimum element in remaining # unsorted array min_idx = i for j in range(i+1, len(A)): if A[min_idx] \u0026gt; A[j]: min_idx = j # Swap the found minimum element with # the first element\tA[i], A[min_idx] = A[min_idx], A[i] # Driver code to test above print (\u0026quot;Sorted array\u0026quot;) for i in range(len(A)): print(\u0026quot;%d\u0026quot; %A[i]), HEAP SORT # Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap def heapify(arr, n, i): largest = i # Initialize largest as root l = 2 * i + 1\t# left = 2*i + 1 r = 2 * i + 2\t# right = 2*i + 2 # See if left child of root exists and is # greater than root if l \u0026lt; n and arr[i] \u0026lt; arr[l]: largest = l # See if right child of root exists and is # greater than root if r \u0026lt; n and arr[largest] \u0026lt; arr[r]: largest = r # Change root, if needed if largest != i: arr[i],arr[largest] = arr[largest],arr[i] # swap # Heapify the root. heapify(arr, n, largest) # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. # Since last parent will be at ((n//2)-1) we can start at that location. for i in range(n // 2 - 1, -1, -1): heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # swap heapify(arr, i, 0) # Driver code to test above arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) n = len(arr) print (\u0026quot;Sorted array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), QUICK SORT # Python program for implementation of Quicksort Sort # This function takes last element as pivot, places # the pivot element at its correct position in sorted # array, and places all smaller (smaller than pivot) # to left of pivot and all greater elements to right # of pivot def partition(arr, low, high): i = (low-1)\t# index of smaller element pivot = arr[high]\t# pivot for j in range(low, high): # If current element is smaller than or # equal to pivot if arr[j] \u0026lt;= pivot: # increment index of smaller element i = i+1 arr[i], arr[j] = arr[j], arr[i] arr[i+1], arr[high] = arr[high], arr[i+1] return (i+1) # The main function that implements QuickSort # arr[] --\u0026gt; Array to be sorted, # low --\u0026gt; Starting index, # high --\u0026gt; Ending index # Function to do Quick sort def quickSort(arr, low, high): if len(arr) == 1: return arr if low \u0026lt; high: # pi is partitioning index, arr[p] is now # at right place pi = partition(arr, low, high) # Separately sort elements before # partition and after partition quickSort(arr, low, pi-1) quickSort(arr, pi+1, high) # Driver code to test above arr = [10, 7, 8, 9, 1, 5] n = len(arr) quickSort(arr, 0, n-1) print(\u0026quot;Sorted array is:\u0026quot;) for i in range(n): print(\u0026quot;%d\u0026quot; % arr[i]), MERGE SORT # Python program for implementation of MergeSort # Merges two subarrays of arr[]. # First subarray is arr[l..m] # Second subarray is arr[m+1..r] def merge(arr, l, m, r): n1 = m - l + 1 n2 = r- m # create temp arrays L = [0] * (n1) R = [0] * (n2) # Copy data to temp arrays L[] and R[] for i in range(0 , n1): L[i] = arr[l + i] for j in range(0 , n2): R[j] = arr[m + 1 + j] # Merge the temp arrays back into arr[l..r] i = 0\t# Initial index of first subarray j = 0\t# Initial index of second subarray k = l\t# Initial index of merged subarray while i \u0026lt; n1 and j \u0026lt; n2 : if L[i] \u0026lt;= R[j]: arr[k] = L[i] i += 1 else: arr[k] = R[j] j += 1 k += 1 # Copy the remaining elements of L[], if there # are any while i \u0026lt; n1: arr[k] = L[i] i += 1 k += 1 # Copy the remaining elements of R[], if there # are any while j \u0026lt; n2: arr[k] = R[j] j += 1 k += 1 # l is for left index and r is right index of the # sub-array of arr to be sorted def mergeSort(arr,l,r): if l \u0026lt; r: # Same as (l+r)//2, but avoids overflow for # large l and h m = (l+(r-1))//2 # Sort first and second halves mergeSort(arr, l, m) mergeSort(arr, m+1, r) merge(arr, l, m, r) # Driver code to test above arr = [12, 11, 13, 5, 6, 7] n = len(arr) print (\u0026quot;Given array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), mergeSort(arr,0,n-1) print (\u0026quot;\\n\\nSorted array is\u0026quot;) for i in range(n): print (\u0026quot;%d\u0026quot; %arr[i]), ","permalink":"https://elvisfinol.github.io/blog/posts/algorithms-pseudo-codes/","summary":"SEARCHING ALGORITHMS BINARY SEARCH binarySearch(input, searchValue) { lower = 0 upper = input.len - 1 white(upper \u0026gt;= lower) { mid = (upper + lower) / 2 if (input[mid] == searchValue) return True elif (searchValue \u0026lt; input[mid]) upper = mid - 1 else: low = mid + 1 } return false } LINEAR SEARCH linearSearch(input, searchValue) { for (i=0 to input.lenght - 1) { if (input[i] == searchValue) return true } return false } JUMP SEARCH # Python3 code to implement Jump Search import math def jumpSearch( arr , x , n ): # Finding block size to be jumped step = math.","title":"Algorithms Pseudocode In Python 🧬"},{"content":"Bash is a shell program. A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n Note: A binary is a file that contains the instructions for a program, ie it is a ‘program’ file, rather than a ‘text’ file, or an ‘application’ file (such as a Word document).\n You only need to know that a shell program is a program that allows you to tell the computer what to do. In that way, it’s not much different to many other kinds of programming languages.\n#!/usr/bin/env bash # First line of the script is the shebang which tells the system how to execute # the script: https://en.wikipedia.org/wiki/Shebang_(Unix) # As you already figured, comments start with #. Shebang is also a comment. # Simple hello world example: echo Hello world! # =\u0026gt; Hello world! # Each command starts on a new line, or after a semicolon: echo 'This is the first line'; echo 'This is the second line' # =\u0026gt; This is the first line # =\u0026gt; This is the second line # Declaring a variable looks like this: Variable=\u0026quot;Some string\u0026quot; # But not like this: Variable = \u0026quot;Some string\u0026quot; # =\u0026gt; returns error \u0026quot;Variable: command not found\u0026quot; # Bash will decide that Variable is a command it must execute and give an error # because it can't be found. # Nor like this: Variable= 'Some string' # =\u0026gt; returns error: \u0026quot;Some string: command not found\u0026quot; # Bash will decide that 'Some string' is a command it must execute and give an # error because it can't be found. (In this case the 'Variable=' part is seen # as a variable assignment valid only for the scope of the 'Some string' # command.) # Using the variable: echo $Variable # =\u0026gt; Some string echo \u0026quot;$Variable\u0026quot; # =\u0026gt; Some string echo '$Variable' # =\u0026gt; $Variable # When you use the variable itself — assign it, export it, or else — you write # its name without $. If you want to use the variable's value, you should use $. # Note that ' (single quote) won't expand the variables! # Parameter expansion ${ }: echo ${Variable} # =\u0026gt; Some string # This is a simple usage of parameter expansion # Parameter Expansion gets a value from a variable. # It \u0026quot;expands\u0026quot; or prints the value # During the expansion time the value or parameter can be modified # Below are other modifications that add onto this expansion # String substitution in variables echo ${Variable/Some/A} # =\u0026gt; A string # This will substitute the first occurrence of \u0026quot;Some\u0026quot; with \u0026quot;A\u0026quot; # Substring from a variable Length=7 echo ${Variable:0:Length} # =\u0026gt; Some st # This will return only the first 7 characters of the value echo ${Variable: -5} # =\u0026gt; tring # This will return the last 5 characters (note the space before -5) # String length echo ${#Variable} # =\u0026gt; 11 # Indirect expansion OtherVariable=\u0026quot;Variable\u0026quot; echo ${!OtherVariable} # =\u0026gt; Some String # This will expand the value of OtherVariable # Default value for variable echo ${Foo:-\u0026quot;DefaultValueIfFooIsMissingOrEmpty\u0026quot;} # =\u0026gt; DefaultValueIfFooIsMissingOrEmpty # This works for null (Foo=) and empty string (Foo=\u0026quot;\u0026quot;); zero (Foo=0) returns 0. # Note that it only returns default value and doesn't change variable value. # Declare an array with 6 elements array0=(one two three four five six) # Print first element echo $array0 # =\u0026gt; \u0026quot;one\u0026quot; # Print first element echo ${array0[0]} # =\u0026gt; \u0026quot;one\u0026quot; # Print all elements echo ${array0[@]} # =\u0026gt; \u0026quot;one two three four five six\u0026quot; # Print number of elements echo ${#array0[@]} # =\u0026gt; \u0026quot;6\u0026quot; # Print number of characters in third element echo ${#array0[2]} # =\u0026gt; \u0026quot;5\u0026quot; # Print 2 elements starting from forth echo ${array0[@]:3:2} # =\u0026gt; \u0026quot;four five\u0026quot; # Print all elements. Each of them on new line. for i in \u0026quot;${array0[@]}\u0026quot;; do echo \u0026quot;$i\u0026quot; done # Brace Expansion { } # Used to generate arbitrary strings echo {1..10} # =\u0026gt; 1 2 3 4 5 6 7 8 9 10 echo {a..z} # =\u0026gt; a b c d e f g h i j k l m n o p q r s t u v w x y z # This will output the range from the start value to the end value # Built-in variables: # There are some useful built-in variables, like echo \u0026quot;Last program's return value: $?\u0026quot; echo \u0026quot;Script's PID: $$\u0026quot; echo \u0026quot;Number of arguments passed to script: $#\u0026quot; echo \u0026quot;All arguments passed to script: $@\u0026quot; echo \u0026quot;Script's arguments separated into different variables: $1 $2...\u0026quot; # Now that we know how to echo and use variables, # let's learn some of the other basics of bash! # Our current directory is available through the command `pwd`. # `pwd` stands for \u0026quot;print working directory\u0026quot;. # We can also use the built-in variable `$PWD`. # Observe that the following are equivalent: echo \u0026quot;I'm in $(pwd)\u0026quot; # execs `pwd` and interpolates output echo \u0026quot;I'm in $PWD\u0026quot; # interpolates the variable # If you get too much output in your terminal, or from a script, the command # `clear` clears your screen clear # Ctrl-L also works for clearing output # Reading a value from input: echo \u0026quot;What's your name?\u0026quot; read Name # Note that we didn't need to declare a new variable echo Hello, $Name! # We have the usual if structure: # use `man test` for more info about conditionals if [ $Name != $USER ] then echo \u0026quot;Your name isn't your username\u0026quot; else echo \u0026quot;Your name is your username\u0026quot; fi # True if the value of $Name is not equal to the current user's login username # NOTE: if $Name is empty, bash sees the above condition as: if [ != $USER ] # which is invalid syntax # so the \u0026quot;safe\u0026quot; way to use potentially empty variables in bash is: if [ \u0026quot;$Name\u0026quot; != $USER ] ... # which, when $Name is empty, is seen by bash as: if [ \u0026quot;\u0026quot; != $USER ] ... # which works as expected # There is also conditional execution echo \u0026quot;Always executed\u0026quot; || echo \u0026quot;Only executed if first command fails\u0026quot; # =\u0026gt; Always executed echo \u0026quot;Always executed\u0026quot; \u0026amp;\u0026amp; echo \u0026quot;Only executed if first command does NOT fail\u0026quot; # =\u0026gt; Always executed # =\u0026gt; Only executed if first command does NOT fail # To use \u0026amp;\u0026amp; and || with if statements, you need multiple pairs of square brackets: if [ \u0026quot;$Name\u0026quot; == \u0026quot;Steve\u0026quot; ] \u0026amp;\u0026amp; [ \u0026quot;$Age\u0026quot; -eq 15 ] then echo \u0026quot;This will run if $Name is Steve AND $Age is 15.\u0026quot; fi if [ \u0026quot;$Name\u0026quot; == \u0026quot;Daniya\u0026quot; ] || [ \u0026quot;$Name\u0026quot; == \u0026quot;Zach\u0026quot; ] then echo \u0026quot;This will run if $Name is Daniya OR Zach.\u0026quot; fi # There is also the `=~` operator, which tests a string against a Regex pattern: Email=me@example.com if [[ \u0026quot;$Email\u0026quot; =~ [a-z]+@[a-z]{2,}\\.(com|net|org) ]] then echo \u0026quot;Valid email!\u0026quot; fi # Note that =~ only works within double [[ ]] square brackets, # which are subtly different from single [ ]. # See https://www.gnu.org/software/bash/manual/bashref.html#Conditional-Constructs for more on this. # Redefine command `ping` as alias to send only 5 packets alias ping='ping -c 5' # Escape the alias and use command with this name instead \\ping 192.168.1.1 # Print all aliases alias -p # Expressions are denoted with the following format: echo $(( 10 + 5 )) # =\u0026gt; 15 # Unlike other programming languages, bash is a shell so it works in the context # of a current directory. You can list files and directories in the current # directory with the ls command: ls # Lists the files and subdirectories contained in the current directory # This command has options that control its execution: ls -l # Lists every file and directory on a separate line ls -t # Sorts the directory contents by last-modified date (descending) ls -R # Recursively `ls` this directory and all of its subdirectories # Results of the previous command can be passed to the next command as input. # The `grep` command filters the input with provided patterns. # That's how we can list .txt files in the current directory: ls -l | grep \u0026quot;\\.txt\u0026quot; # Use `cat` to print files to stdout: cat file.txt # We can also read the file using `cat`: Contents=$(cat file.txt) # \u0026quot;\\n\u0026quot; prints a new line character # \u0026quot;-e\u0026quot; to interpret the newline escape characters as escape characters echo -e \u0026quot;START OF FILE\\n$Contents\\nEND OF FILE\u0026quot; # =\u0026gt; START OF FILE # =\u0026gt; [contents of file.txt] # =\u0026gt; END OF FILE # Use `cp` to copy files or directories from one place to another. # `cp` creates NEW versions of the sources, # so editing the copy won't affect the original (and vice versa). # Note that it will overwrite the destination if it already exists. cp srcFile.txt clone.txt cp -r srcDirectory/ dst/ # recursively copy # Look into `scp` or `sftp` if you plan on exchanging files between computers. # `scp` behaves very similarly to `cp`. # `sftp` is more interactive. # Use `mv` to move files or directories from one place to another. # `mv` is similar to `cp`, but it deletes the source. # `mv` is also useful for renaming files! mv s0urc3.txt dst.txt # sorry, l33t hackers... # Since bash works in the context of a current directory, you might want to # run your command in some other directory. We have cd for changing location: cd ~ # change to home directory cd # also goes to home directory cd .. # go up one directory # (^^say, from /home/username/Downloads to /home/username) cd /home/username/Documents # change to specified directory cd ~/Documents/.. # still in home directory..isn't it?? cd - # change to last directory # =\u0026gt; /home/username/Documents # Use subshells to work across directories (echo \u0026quot;First, I'm here: $PWD\u0026quot;) \u0026amp;\u0026amp; (cd someDir; echo \u0026quot;Then, I'm here: $PWD\u0026quot;) pwd # still in first directory # Use `mkdir` to create new directories. mkdir myNewDir # The `-p` flag causes new intermediate directories to be created as necessary. mkdir -p myNewDir/with/intermediate/directories # if the intermediate directories didn't already exist, running the above # command without the `-p` flag would return an error # You can redirect command input and output (stdin, stdout, and stderr). # Read from stdin until ^EOF$ and overwrite hello.py with the lines # between \u0026quot;EOF\u0026quot;: cat \u0026gt; hello.py \u0026lt;\u0026lt; EOF #!/usr/bin/env python from __future__ import print_function import sys print(\u0026quot;#stdout\u0026quot;, file=sys.stdout) print(\u0026quot;#stderr\u0026quot;, file=sys.stderr) for line in sys.stdin: print(line, file=sys.stdout) EOF # Variables will be expanded if the first \u0026quot;EOF\u0026quot; is not quoted # Run the hello.py Python script with various stdin, stdout, and # stderr redirections: python hello.py \u0026lt; \u0026quot;input.in\u0026quot; # pass input.in as input to the script python hello.py \u0026gt; \u0026quot;output.out\u0026quot; # redirect output from the script to output.out python hello.py 2\u0026gt; \u0026quot;error.err\u0026quot; # redirect error output to error.err python hello.py \u0026gt; \u0026quot;output-and-error.log\u0026quot; 2\u0026gt;\u0026amp;1 # redirect both output and errors to output-and-error.log python hello.py \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # redirect all output and errors to the black hole, /dev/null, i.e., no output # The output error will overwrite the file if it exists, # if you want to append instead, use \u0026quot;\u0026gt;\u0026gt;\u0026quot;: python hello.py \u0026gt;\u0026gt; \u0026quot;output.out\u0026quot; 2\u0026gt;\u0026gt; \u0026quot;error.err\u0026quot; # Overwrite output.out, append to error.err, and count lines: info bash 'Basic Shell Features' 'Redirections' \u0026gt; output.out 2\u0026gt;\u0026gt; error.err wc -l output.out error.err # Run a command and print its file descriptor (e.g. /dev/fd/123) # see: man fd echo \u0026lt;(echo \u0026quot;#helloworld\u0026quot;) # Overwrite output.out with \u0026quot;#helloworld\u0026quot;: cat \u0026gt; output.out \u0026lt;(echo \u0026quot;#helloworld\u0026quot;) echo \u0026quot;#helloworld\u0026quot; \u0026gt; output.out echo \u0026quot;#helloworld\u0026quot; | cat \u0026gt; output.out echo \u0026quot;#helloworld\u0026quot; | tee output.out \u0026gt;/dev/null # Cleanup temporary files verbosely (add '-i' for interactive) # WARNING: `rm` commands cannot be undone rm -v output.out error.err output-and-error.log rm -r tempDir/ # recursively delete # You can install the `trash-cli` Python package to have `trash` # which puts files in the system trash and doesn't delete them directly # see https://pypi.org/project/trash-cli/ if you want to be careful # Commands can be substituted within other commands using $( ): # The following command displays the number of files and directories in the # current directory. echo \u0026quot;There are $(ls | wc -l) items here.\u0026quot; # The same can be done using backticks `` but they can't be nested - # the preferred way is to use $( ). echo \u0026quot;There are `ls | wc -l` items here.\u0026quot; # Bash uses a `case` statement that works similarly to switch in Java and C++: case \u0026quot;$Variable\u0026quot; in # List patterns for the conditions you want to meet 0) echo \u0026quot;There is a zero.\u0026quot;;; 1) echo \u0026quot;There is a one.\u0026quot;;; *) echo \u0026quot;It is not null.\u0026quot;;; # match everything esac # `for` loops iterate for as many arguments given: # The contents of $Variable is printed three times. for Variable in {1..3} do echo \u0026quot;$Variable\u0026quot; done # =\u0026gt; 1 # =\u0026gt; 2 # =\u0026gt; 3 # Or write it the \u0026quot;traditional for loop\u0026quot; way: for ((a=1; a \u0026lt;= 3; a++)) do echo $a done # =\u0026gt; 1 # =\u0026gt; 2 # =\u0026gt; 3 # They can also be used to act on files.. # This will run the command `cat` on file1 and file2 for Variable in file1 file2 do cat \u0026quot;$Variable\u0026quot; done # ..or the output from a command # This will `cat` the output from `ls`. for Output in $(ls) do cat \u0026quot;$Output\u0026quot; done # Bash can also accept patterns, like this to `cat` # all the Markdown files in current directory for Output in ./*.markdown do cat \u0026quot;$Output\u0026quot; done # while loop: while [ true ] do echo \u0026quot;loop body here...\u0026quot; break done # =\u0026gt; loop body here... # You can also define functions # Definition: function foo () { echo \u0026quot;Arguments work just like script arguments: $@\u0026quot; echo \u0026quot;And: $1 $2...\u0026quot; echo \u0026quot;This is a function\u0026quot; return 0 } # Call the function `foo` with two arguments, arg1 and arg2: foo arg1 arg2 # =\u0026gt; Arguments work just like script arguments: arg1 arg2 # =\u0026gt; And: arg1 arg2... # =\u0026gt; This is a function # or simply bar () { echo \u0026quot;Another way to declare functions!\u0026quot; return 0 } # Call the function `bar` with no arguments: bar # =\u0026gt; Another way to declare functions! # Calling your function foo \u0026quot;My name is\u0026quot; $Name # There are a lot of useful commands you should learn: # prints last 10 lines of file.txt tail -n 10 file.txt # prints first 10 lines of file.txt head -n 10 file.txt # sort file.txt's lines sort file.txt # report or omit repeated lines, with -d it reports them uniq -d file.txt # prints only the first column before the ',' character cut -d ',' -f 1 file.txt # replaces every occurrence of 'okay' with 'great' in file.txt # (regex compatible) sed -i 's/okay/great/g' file.txt # be aware that this -i flag means that file.txt will be changed # -i or --in-place erase the input file (use --in-place=.backup to keep a back-up) # print to stdout all lines of file.txt which match some regex # The example prints lines which begin with \u0026quot;foo\u0026quot; and end in \u0026quot;bar\u0026quot; grep \u0026quot;^foo.*bar$\u0026quot; file.txt # pass the option \u0026quot;-c\u0026quot; to instead print the number of lines matching the regex grep -c \u0026quot;^foo.*bar$\u0026quot; file.txt # Other useful options are: grep -r \u0026quot;^foo.*bar$\u0026quot; someDir/ # recursively `grep` grep -n \u0026quot;^foo.*bar$\u0026quot; file.txt # give line numbers grep -rI \u0026quot;^foo.*bar$\u0026quot; someDir/ # recursively `grep`, but ignore binary files # perform the same initial search, but filter out the lines containing \u0026quot;baz\u0026quot; grep \u0026quot;^foo.*bar$\u0026quot; file.txt | grep -v \u0026quot;baz\u0026quot; # if you literally want to search for the string, # and not the regex, use `fgrep` (or `grep -F`) fgrep \u0026quot;foobar\u0026quot; file.txt # The `trap` command allows you to execute a command whenever your script # receives a signal. Here, `trap` will execute `rm` if it receives any of the # three listed signals. trap \u0026quot;rm $TEMP_FILE; exit\u0026quot; SIGHUP SIGINT SIGTERM # `sudo` is used to perform commands as the superuser # usually it will ask interactively the password of superuser NAME1=$(whoami) NAME2=$(sudo whoami) echo \u0026quot;Was $NAME1, then became more powerful $NAME2\u0026quot; # Read Bash shell built-ins documentation with the bash `help` built-in: help help help help for help return help source help . # Read Bash manpage documentation with `man` apropos bash man 1 bash man bash # Read info documentation with `info` (`?` for help) apropos info | grep '^info.*(' man info info info info 5 info # Read bash info documentation: info bash info bash 'Bash Features' info bash 6 info --apropos bash Source https://learnxinyminutes.com/docs/bash/\n","permalink":"https://elvisfinol.github.io/blog/posts/bash-in-minutes/","summary":"Bash is a shell program. A shell program is typically an executable binary that takes commands that you type and (once you hit return), translates those commands into (ultimately) system calls to the Operating System API.\n Note: A binary is a file that contains the instructions for a program, ie it is a ‘program’ file, rather than a ‘text’ file, or an ‘application’ file (such as a Word document).","title":"BASH in minutes"},{"content":"Table content  Concept The benefits of a Load Balancer How does it work Type of Load Balancer Load Balancer locations Algorithms  Concept A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers. It helps scale horizontally across an ever-increasing number of servers.\n⇧ back to topThe benefits of a Load Balancer  Reduced the work-load on an individual server. Large amount of work done in same time due to concurrency. Increased performance of your application because of faster response. No single point of failure. In a load balanced environment, if a server crashes the application is still up and served by the other servers in the cluster. When appropriate load balancing algorithm is used, it brings optimal and efficient utilization of the resources, as it eliminates the scenario of some server’s resources are getting used than others. Scalability: We can increase or decrease the number of servers on the fly without bringing down the application. Load balancing increases the reliability of your enterprise application. Increased security as the physical servers and IPs are abstract in certain cases.  ⇧ back to topHow does it work  Define IP or DNS name for LB: Administrators define one IP address and/or DNS name for a given application, task, or website, to which all requests will come. This IP address or DNS name is the load balancing server. Add backend pool for LB: The administrator will then enter into the load balancing server the IP addresses of all the actual servers that will be sharing the workload for a given application or task. This pool of available servers is only accessible internally, via the load balancer. Deploy LB: Finally, your load balancer needs to be deployed — either as a proxy, which sits between your app servers and your users worldwide and accepts all traffic, or as a gateway, which assigns a user to a server once and leaves the interaction alone thereafter. Redirect requests: Once the load balancing system is in place, all requests to the application come to the load balancer and are redirected according to the administrator’s preferred algorithm.  ⇧ back to topType of Load Balancer  Network Load Balancing: Network load balancing, as its name suggests, leverages network layer information to decide where to send network traffic. This is accomplished through layer 4 load balancing, which is designed to handle all forms of TCP/UDP traffic. Network load balancing is considered the fastest of all the load balancing solutions, but it tends to fall short when it comes to balancing the distribution of traffic across servers. HTTP(S) Load Balancing: HTTP(S) load balancing is one of the oldest forms of load balancing. This form of load balancing relies on layer 7, which means it operates in the application layer. HTTP load balancing is often dubbed the most flexible type of load balancing because it allows you to form distribution decisions based on any information that comes with an HTTP address. Internal Load Balancing: Internal load balancing is nearly identical to network load balancing but can be leveraged to balance internal infrastructure.   When talking about types of load balancers, it’s also important to note there are hardware load balancers, software load balancers, and virtual load balancers.\n  Hardware Load Balancer: A hardware load balancer, as the name implies, relies on physical, on-premises hardware to distribute application and network traffic. These devices can handle a large volume of traffic but often carry a hefty price tag and are fairly limited in terms of flexibility. Software Load Balancer: A software load balancer comes in two forms—commercial or open-source—and must be installed prior to use. Like cloud-based balancers, these tend to be more affordable than hardware solutions. Virtual Load Balancer: A virtual load balancer differs from software load balancers because it deploys the software of a hardware load balancing device on a virtual machine.  ⇧ back to topLoad Balancer Locations  Between user and web servers (User =\u0026gt; Web Servers) Between web servers and an internal platform layer (application servers, cache servers) (Webservers =\u0026gt; App or Cache servers) Between internal platform layer and database (App or Cache servers =\u0026gt; Database servers)  ⇧ back to topAlgorithms There is a variety of load balancing methods, which use different algorithms best suited for a particular situation.\nRound-robin load balancing is one of the simplest and most used load balancing algorithms. Client requests are distributed to application servers in rotation. For example, if you have three application servers: the first client request to the first application server in the list, the second client request to the second application server, the third client request to the third application server, the fourth to the first application server and so on. This load balancing algorithm does not take into consideration the characteristics of the application servers i.e. it assumes that all application servers are the same with the same availability, computing and load handling characteristics.\nWeighted Round Robin builds on the simple Round-robin load balancing algorithm to account for differing application server characteristics. The administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability.\nIf application server #1 is twice as powerful as application server #2 (and application server #3), application server #1 is provisioned with a higher weight and application server #2 and #3 get the same weight. If there five (5) sequential client requests, the first two (2) go to application server #1, the third (3) goes to application server #2, the fourth (4) to application server #3 and the fifth (5) to application server #1.\nLeast Connection load balancing is a dynamic load balancing algorithm where client requests are distributed to the application server with the least number of active connections at the time the client request is received. In cases where application servers have similar specifications, an application server may be overloaded due to longer lived connections; this algorithm takes the active connection load into consideration.\nWeighted Least Connection builds on the Least Connection load balancing algorithm to account for differing application server characteristics. The administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability. The LoadMaster is making the load balancing criteria based on active connections and application server weighting.\nResource Based (Adaptive) is a load balancing algorithm requires an agent to be installed on the application server that reports on its current load to the load balancer. The installed agent monitors the application servers availability status and resources. The load balancer queries the output from the agent to aid in load balancing decisions.\nSDN Adaptive is a load balancing algorithm that combines knowledge from Layers 2, 3, 4 and 7 and input from an SDN Controller to make more optimized traffic distribution decisions. This allows information about the status of the servers, the status of the applications running on them, the health of the network infrastructure, and the level of congestion on the network to all play a part in the load balancing decision making.\nFixed Weighting is a load balancing algorithm where the administrator assigns a weight to each application server based on criteria of their choosing to demonstrate the application servers traffic-handling capability. The application server with the highest weigh will receive all of the traffic. If the application server with the highest weight fails, all traffic will be directed to the next highest weight application server.\nWeighted Response Time is a load balancing algorithm where the response times of the application servers determines which application server receives the next request. The application server response time to a health check is used to calculate the application server weights. The application server that is responding the fastest receives the next request.\nSource IP hash load balancing algorithm that combines source and destination IP addresses of the client and server to generate a unique hash key. The key is used to allocate the client to a particular server. As the key can be regenerated if the session is broken, the client request is directed to the same server it was using previously. This is useful if it’s important that a client should connect to a session that is still active after a disconnection.\nURL Hash is a load balancing algorithm to distribute writes evenly across multiple sites and sends all reads to the site owning the object.\n⇧ back to topReferences  https://www.thegeekstuff.com/2016/01/load-balancer-intro/ https://avinetworks.com/glossary/server-load-balancer/ https://medium.com/must-know-computer-science/system-design-load-balancing-1c2e7675fc27 https://lumecloud.com/what-does-a-load-balancer-do/  ","permalink":"https://elvisfinol.github.io/blog/posts/load-balancing-deep-dive/","summary":"Table content  Concept The benefits of a Load Balancer How does it work Type of Load Balancer Load Balancer locations Algorithms  Concept A load balancer is a device that acts as a reverse proxy and distributes network or application traffic across a number of servers. It helps scale horizontally across an ever-increasing number of servers.\n⇧ back to topThe benefits of a Load Balancer  Reduced the work-load on an individual server.","title":"Load Balancing [Deep Dive]"},{"content":"The following information can help you troubleshoot issues with your Application Load Balancer.\nSuccessful request: Code 200 ✅ Unsuccessfull at client site: 4XX code ❌  Error 400: Bad Request Error 401: Unauthorized Error 403: Forbidden Error 405: Method not allowed Error 408: Request timeout Error 413: Payload too large Error 414: URI too long Error 460: Client close connection Error 463: X-Forwarded For header with \u0026gt; 30 IP (Similar to malformed request)  Unsuccessfull at server site: 5XX code ❌  Error 500: Internal server error would mean some error on the ELB itseft Error 501: Not Implemented Error 502 : Bad Gateway Error 503: Service Unavailable Error 504: Gateway timeout, probably an issue within the server Error 505: Version not supported Error 561: Unauthorized  references https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\n","permalink":"https://elvisfinol.github.io/blog/posts/aws-loadbalancer-error-codes/","summary":"The following information can help you troubleshoot issues with your Application Load Balancer.\nSuccessful request: Code 200 ✅ Unsuccessfull at client site: 4XX code ❌  Error 400: Bad Request Error 401: Unauthorized Error 403: Forbidden Error 405: Method not allowed Error 408: Request timeout Error 413: Payload too large Error 414: URI too long Error 460: Client close connection Error 463: X-Forwarded For header with \u0026gt; 30 IP (Similar to malformed request)  Unsuccessfull at server site: 5XX code ❌  Error 500: Internal server error would mean some error on the ELB itseft Error 501: Not Implemented Error 502 : Bad Gateway Error 503: Service Unavailable Error 504: Gateway timeout, probably an issue within the server Error 505: Version not supported Error 561: Unauthorized  references https://docs.","title":"[CHEAT SHEET 📒] - AWS Application Load Balancer Error Codes"},{"content":"Here you will find several methods to access your EC2 if you lost you SSH key\nIf the instance is EBS backed:  Stop the instance, detach the root volume Attach the root volume to another instance as a data volume Modify the ~/.ssh/authorized_keys file with your new key Move the the volume back to the stopped instance Start the instance and you can SSH into it again  If the instance is EBS (new method 🧙):  Run the AWSSupport-ResetAccess automation document in SSM  Instance Store backed EC2:  You cannot stop the instance (otherwise data is lost) - AWS recommends termination Use Session Manager access and edit the ~/.ssh/authorized_keys file directly  references  Connect to your Linux instance if you lose your private key. AWSSupport-ResetAccess  ","permalink":"https://elvisfinol.github.io/blog/posts/aws-tutorial-method-to-recover-ssh-key/","summary":"Here you will find several methods to access your EC2 if you lost you SSH key\nIf the instance is EBS backed:  Stop the instance, detach the root volume Attach the root volume to another instance as a data volume Modify the ~/.ssh/authorized_keys file with your new key Move the the volume back to the stopped instance Start the instance and you can SSH into it again  If the instance is EBS (new method 🧙):  Run the AWSSupport-ResetAccess automation document in SSM  Instance Store backed EC2:  You cannot stop the instance (otherwise data is lost) - AWS recommends termination Use Session Manager access and edit the ~/.","title":"[How-to] I lost my SSH key for EC2! Steps to get into your instance back"},{"content":"Before start please install Apache HTTP Server on the instance \u0026gt; following the “yum install httpd” command and create a simple index.html file under path /var/www/html/index.html\nAfter that, start httpd agent an validate accessing over the public DNS, you should see your “Hello World”. If you see that on your browser this means that your server is running properly.\nRemember we will catch the following logs. So we need to put both path on the CloudWatch wizard later.\n/var/log/httpd/access_log \u0026amp; /var/log/httpd/error_log\nInstalling the CloudWatch Agent On this step we install the Unified CloudWatch Agent that will allows us to send metrics and logs into CloudWatch. You can store and retrieve configuration into the SSM Parameter Store and allow you to have quick setup for all your instances if you want to have them all configure the same way!\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm\nsudo rpm -U ./amazon-cloudwatch-agent.rpm\nI had already install\nRun the wizard\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard ============================================================= = Welcome to the AWS CloudWatch Agent Configuration Manager = ============================================================= On which OS are you planning to use the agent? 1. linux 2. windows 3. darwin default choice: [1]: 1 Trying to fetch the default region based on ec2 metadata... Are you using EC2 or On-Premises hosts? 1. EC2 2. On-Premises default choice: [1]: 1 Which user are you planning to run the agent? 1. root 2. cwagent 3. others default choice: [1]: 1 Do you want to turn on StatsD daemon? 1. yes 2. no default choice: [1]: 1 Which port do you want StatsD daemon to listen to? default choice: [8125] What is the collect interval for StatsD daemon? 1. 10s 2. 30s 3. 60s default choice: [1]: 1 What is the aggregation interval for metrics collected by StatsD daemon? 1. Do not aggregate 2. 10s 3. 30s 4. 60s default choice: [4]: 6 The value 6 is not valid to this question. Please retry to answer: What is the aggregation interval for metrics collected by StatsD daemon? 1. Do not aggregate 2. 10s 3. 30s 4. 60s default choice: [4]: 4 Do you want to monitor metrics from CollectD? 1. yes 2. no default choice: [1]: 1 Do you want to monitor any host metrics? e.g. CPU, memory, etc. 1. yes 2. no default choice: [1]: 1 Do you want to monitor cpu metrics per core? Additional CloudWatch charges may apply. 1. yes 2. no default choice: [1]: 1 Do you want to add ec2 dimensions (ImageId, InstanceId, InstanceType, AutoScalingGroupName) into all of your metrics if the info is available? 1. yes 2. no default choice: [1]: 1 Would you like to collect your metrics at high resolution (sub-minute resolution)? This enables sub-minute resolution for all metrics, but you can customize for specific metrics in the output json file. 1. 1s 2. 10s 3. 30s 4. 60s default choice: [4]: 4 Which default metrics config do you want? 1. Basic 2. Standard 3. Advanced 4. None default choice: [1]: 1 Current config as follows: { \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;collectd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60 }, \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 }, \u0026quot;statsd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60, \u0026quot;metrics_collection_interval\u0026quot;: 10, \u0026quot;service_address\u0026quot;: \u0026quot;:8125\u0026quot; } } } } Are you satisfied with the above config? Note: it can be manually customized after the wizard completes to add additional items. 1. yes 2. no default choice: [1]: 1 Do you have any existing CloudWatch Log Agent (http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html) configuration file to import for migration? 1. yes 2. no default choice: [2]: 2 Do you want to monitor any log files? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/access_log Log group name: default choice: [access_log] Log stream name: default choice: [{instance_id}] Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/error_log Log group name: default choice: [error_log] Log stream name: default choice: [{instance_id}] Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 2 Saved config file to /opt/aws/amazon-cloudwatch-agent/bin/config.json successfully. Current config as follows: { \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;root\u0026quot; }, \u0026quot;logs\u0026quot;: { \u0026quot;logs_collected\u0026quot;: { \u0026quot;files\u0026quot;: { \u0026quot;collect_list\u0026quot;: [ { \u0026quot;file_path\u0026quot;: \u0026quot;/var/log/httpd/access_log\u0026quot;, \u0026quot;log_group_name\u0026quot;: \u0026quot;access_log\u0026quot;, \u0026quot;log_stream_name\u0026quot;: \u0026quot;{instance_id}\u0026quot; }, { \u0026quot;file_path\u0026quot;: \u0026quot;/var/log/httpd/error_log\u0026quot;, \u0026quot;log_group_name\u0026quot;: \u0026quot;error_log\u0026quot;, \u0026quot;log_stream_name\u0026quot;: \u0026quot;{instance_id}\u0026quot; } ] } } }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;collectd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60 }, \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 }, \u0026quot;statsd\u0026quot;: { \u0026quot;metrics_aggregation_interval\u0026quot;: 60, \u0026quot;metrics_collection_interval\u0026quot;: 10, \u0026quot;service_address\u0026quot;: \u0026quot;:8125\u0026quot; } } } } Please check the above content of the config. The config file is also located at /opt/aws/amazon-cloudwatch-agent/bin/config.json. Edit it manually if needed. Do you want to store the config in the SSM parameter store? 1. yes 2. no default choice: [1]: 1 What parameter store name do you want to use to store your config? (Use 'AmazonCloudWatch-' prefix if you use our managed AWS policy) default choice: [AmazonCloudWatch-linux] Trying to fetch the default region based on ec2 metadata... Which region do you want to store the config in the parameter store? default choice: [us-east-1] Which AWS credential should be used to send json config to parameter store? 1. ASIA4ZKOFOINW3ZORZGS(From SDK) 2. Other default choice: [1]: Please make sure the creds you used have the right permissions configured for SSM access. Which AWS credential should be used to send json config to parameter store? 1. ASIA4ZKOFOINW3ZORZGS(From SDK) 2. Other default choice: [1]: Successfully put config to parameter store AmazonCloudWatch-linux. Program exits now.  You can see JSON config is already on the Parameter Store. So any other EC2 instances will bootup and fetch the value of this config.\nIn order to point to the SSM Parameter you have two options:\nFetch the config\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux  Reading directly JSON file\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: us-east-1 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2021/01/22 15:04:52 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase failed ======== Error Log ======== 2021-01-22T15:04:52Z E! [telegraf] Error running agent: Error parsing /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml, open /usr/share/collectd/types.db: no such file or directory [root@ip-172-31-55-22 ec2-user]#  We successfully fetch the config. Probably you will see an error that /usr/share/collectd/types.db is missing.\nTo solve this create the folders/file and re-run the agent\n[root@ip-172-31-55-22 ec2-user]# mkdir -p /usr/share/collectd/ [root@ip-172-31-55-22 ec2-user]# touch /usr/share/collectd/types.db [root@ip-172-31-55-22 ec2-user]# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c ssm:AmazonCloudWatch-linux ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: us-east-1 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2021/01/22 15:10:13 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase succeeded Configuration validation succeeded amazon-cloudwatch-agent has already been stopped Created symlink from /etc/systemd/system/multi-user.target.wants/amazon-cloudwatch-agent.service to /etc/systemd/system/amazon-cloudwatch-agent.service. Redirecting to /bin/systemctl restart amazon-cloudwatch-agent.service [root@ip-172-31-55-22 ec2-user]#  Now agent is WORKING and you should start seeing CloudWatchLogs and Metrics! 💪 Go to CloudWatch and check it out:\nLogs\nMetrics\nreferences\n Installing and Running the CloudWatch Agent on Your Servers  ","permalink":"https://elvisfinol.github.io/blog/posts/how-to-install-cloudwatch-agent-on-ec2/","summary":"Before start please install Apache HTTP Server on the instance \u0026gt; following the “yum install httpd” command and create a simple index.html file under path /var/www/html/index.html\nAfter that, start httpd agent an validate accessing over the public DNS, you should see your “Hello World”. If you see that on your browser this means that your server is running properly.\nRemember we will catch the following logs. So we need to put both path on the CloudWatch wizard later.","title":"Tutorial - How to install CloudWatch Agent on EC2 Instances?"},{"content":"Hey there! Here you will find this cheat sheet which I prepared to show, all linux commands which I use to day a day, in my actual role as System Engineer.\nTable content  Help Users Network Files-Folders Remote  More coming soon! Stay tuned\u0026hellip;\n Permissions Editors Differences Packages Compressor   Help Help commands\n man manual page (wide) whatis short description apropos related help  ⇧ back to topUsers👤 Change and info\n whoami current user su switch to user [root] visudo edit sudoers [vi+sudo]  System users\n useradd create/update user  adduser friendly add user   userdel delete user  deluser friendly del user   usermod modify user account groups group members passwd change password Islogins show known users  Logged users\n who current logged users  w current logged users \u0026amp; data   users current logged users last last logged users \u0026amp; reboots lastb last bad logins lastlog recent login users  ⇧ back to topNetwork🌐 Net configuration\n ifconfig config ip/net features  ip new ifconfig tool   dhclient DHCP client  DNS and domains\n ping send ICMP to hosts nslookup query DNS lookup  dig DNS lookup utility   whois whois domain name or ip  Trace route\n traceroute print route packets tracepath trace path  mtr network diagnostic tool    Network tools\n nmap network security scanner nc cat via network connection ss show sockets statistics  Network monitoring\n bmon bandwidth monitor iftop interface network monitor nethogs net monitor by process wondershaper bandwidth limit Iptraf-ng network monitor tcpdump network activity dump netstat print network statistics  Mac address\n arp show mac/ip address cache arping ping mac address  Firewall\n  iptables ip packet filter \u0026amp; NAT\n shorewall firewall for iptables ufw firewall for iptables  ⇧ back to top  Files-Folders Folders (Directories)\n mkdir create dir  -p all dirs   pwd current dir ls list files \u0026amp; dirs  -l long data -h human -R recursive   cd change to dir pushd / popd directory stack autojump smart jump to dirs tree list files in tree format  File handling\n file show file type touch update/create mv move/rename files  -f force -u update   cp copy files  -f force -r recursive   rm remove files ln make link to file stat filesytem stats  Fine files\n  type display command type\n  Find search files in a dir\n locate search files in database updatedb update file database    whereis locate binary/manpage\n  which get binary file pathname\n⇧ back to top  Remote  telnet telnet connection ftp file transfer connect  ssh remote connection   sftp connect ftp via ssh sshfs connect disk via ssh  SCP/SSH SYNTAX\nscp user@ip:/folder remote ⇧ back to topMore coming soon! Stay tuned\u0026hellip;\n","permalink":"https://elvisfinol.github.io/blog/posts/my-linux-terminal-cheatsheet/","summary":"Hey there! Here you will find this cheat sheet which I prepared to show, all linux commands which I use to day a day, in my actual role as System Engineer.\nTable content  Help Users Network Files-Folders Remote  More coming soon! Stay tuned\u0026hellip;\n Permissions Editors Differences Packages Compressor   Help Help commands\n man manual page (wide) whatis short description apropos related help  ⇧ back to topUsers👤 Change and info","title":"[CHEAT SHEET 📒] - My Linux Terminal"},{"content":"You have a theme you added as a git submodule and you recently re-cloned your project. In order to fix this, your submodule needs to be re-downloaded as well.\nRun the following commands:\ngit submodule init   git submodule update   hugo server   Then your project will load without errors.\n","permalink":"https://elvisfinol.github.io/blog/posts/how-to-fix-layout-no-found-in-hugo/","summary":"You have a theme you added as a git submodule and you recently re-cloned your project. In order to fix this, your submodule needs to be re-downloaded as well.\nRun the following commands:\ngit submodule init   git submodule update   hugo server   Then your project will load without errors.","title":"[Solved] How to fix the error found no layout file for HTML for page in Hugo?"},{"content":"Al utilizar el protocolo SSH, puedes conectar y autenticar a servidores remotos y servicios. La ventaja para el caso de GitHub es que no necesitas suministrar “username” y “personal token” en cada deploy.\nComandos Paso 1: Generar llave SSH\nAsegúrate que Git Bash esta abierto. Para generar una llave SSH ejecuta este comando:\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;ejemplo@ejemplo.com\u0026quot;\n(NO olvides reemplazar el email “ejemplo@ejemplo.com” por tu email real)\nPaso 2: Uso de llave ** Ahora que la llave esta generada, vamos a utilizarla! Primero debemos iniciar el agente de SSH ejecutando:\n$ eval $(ssh-agent -s)\nUna vez iniciado el agente, vamos a agregar la llave que hemos generado. Ten en cuenta que si seleccionas un path diferente al default, debes cambiarlo en el comando:\n$ ssh-add ~/.ssh/id_rsa\n****Paso 3: Agregar llave SSH en GitHub\nAhora que tenemos la llave ssh configurada en nuestra PC, procedemos agregarla en GitHub. Para ello puedes ejecutar este comando para copiarla y luego pegarla en GitHub.\n$ clip \u0026lt; ~/.ssh/id_rsa.pub\nVe a las Settings en tu GitHub, en el sidebar haz click en “SSH and GPG Keys”\nHaz click en “New SSH Key”\nEn title puedes colocar la descripción, por ejemplo “Mi computador personal” y seguidamente vas a pegar tu llave en “Key”.\nLuego haces click en “Add SSH key”, te pedirá las credenciales de tu cuenta en GitHub. Al llegar a este punto tu llave ha sido agregada y puedes empezar a hacer push desde tu repositorio remoto a GitHub. 🙂\n","permalink":"https://elvisfinol.github.io/blog/posts/configurando-ssh-con-github-windows/","summary":"Al utilizar el protocolo SSH, puedes conectar y autenticar a servidores remotos y servicios. La ventaja para el caso de GitHub es que no necesitas suministrar “username” y “personal token” en cada deploy.\nComandos Paso 1: Generar llave SSH\nAsegúrate que Git Bash esta abierto. Para generar una llave SSH ejecuta este comando:\n$ ssh-keygen -t rsa -b 4096 -C \u0026quot;ejemplo@ejemplo.com\u0026quot;\n(NO olvides reemplazar el email “ejemplo@ejemplo.com” por tu email real)","title":"Tutorial - Configurando SSH con GitHub (Windows)"},{"content":"Frameworks, guides and several tools to help you improve your writing CSS.\nTable content  Frameworks Tools Preprocessors Reset and Normalize Design Inspiration Illustrations Placeholder Games Icons  Frameworks🎨 CSS framework gives web developers a basic structure, which includes grid, interactive UI patterns, web typography, tooltips, buttons, form elements, icons. This structure helps web developers to start quickly and efficiently when they are designing a website or web applications.\n awsm.css - Simple CSS library for semantic HTML markup. Bonsai - A complete Utility First CSS Framework for less than 50kb. Bootstrap - The most popular HTML, CSS, and JS framework. Bulma - A modern CSS framework based on Flexbox. Also has Sass import for modification. Butter Cake - A Modern Lightweight Front End CSS framework for faster and easier web development. Charts.css - CSS data visualization framework. Chota - A responsive, customizable micro-framework (3kb) with helpful utilities and a grid system. Cirrus - A fully responsive and comprehensive CSS framework with beautiful controls and simplistic structure. eFrolic - CSS framework which without using JavaScript is interactive and animated. Foundation - advanced responsive front-end framework. Gralig - A modest, grayish CSS library. Halfmoon - A responsive front-end framework with a built-in dark mode. Hasser CSS - A lightweight (12k, not minified) but useful CSS framework with flexible Grid, Hero and more. inuit.css - Powerful, scalable, Sass-based, BEM, OOCSS framework. material-components-web - Modular and customizable Material Design UI components for the web. Materialize - A modern responsive front-end framework based on Material Design. Milligram - A minimalist CSS framework. Numl - An HTML-based language and design system that lets you create responsive and accessible high-quality web interfaces with any look. Pure.css - A set of small, responsive CSS modules that you can use in every web project. Semantic UI - Powerful framework that uses human-friendly HTML. Shorthand Framework - Feature rich CSS framework for the new decade. Spectre.css - A lightweight, responsive and modern CSS framework. Strawberry - A set of common flexbox utilities focused on making your life easier and faster with nested flexboxes. Tachyons - Functional CSS for humans. Tacit - CSS framework for dummies with zero skills in graphic design. tailwindcss - A utility-first CSS framework for rapid UI development. UIkit - A lightweight and modular front-end framework. unsemantic - Fluid grid for mobile, tablet, and desktop. Wing - A Minimal, Lightweight, Responsive framework.  ⇧ back to topTools🔧  Codepen - The best place to build, test, and discover front-end code. Pleese Play - All the annoying CSS stuff we don\u0026rsquo;t want to do in 1 tool. CCSmatic - The ultimate CSS tools for web designers.  ⇧ back to topPreprocessors⚙️ A CSS preprocessor is a program that lets you generate CSS from the preprocessor\u0026rsquo;s own unique syntax. There are many CSS preprocessors to choose from, however most CSS preprocessors will add some features that don\u0026rsquo;t exist in pure CSS, such as mixin, nesting selector, inheritance selector, and so on. These features make the CSS structure more readable and easier to maintain.\n LESS - Backwards compatible with CSS, and the extra features it adds use existing CSS syntax. PostCSS - Transforming CSS with JS plugins. Sass - Mature, stable, and powerful professional-grade CSS extension language. STYLIS - Light-weight CSS preprocessor. Stylus - Expressive, robust, feature-rich CSS language built for NodeJs.  ⇧ back to topReset and Normalize  Normalize - A set of CSS rules that provide better cross-browser consistency in the default styling of HTML elements. Normalize-OpenType - Adds OpenType features—ligatures, kerning, and more—to Normalize.css. MiniReset.css - A tiny modern CSS reset. sanitize.css - A set of CSS rules that style with today’s best practices out-of-the-box. unstyle.css - Specialised stylesheet for removing user agent styles, style the web with your baseline. reset.css - CSS Tools: Reset CSS.  ⇧ back to topDesign Inspiration  Behance - is a social media platform owned by Adobe \u0026ldquo;to showcase and discover creative work\u0026rdquo; Dribbble - is a self-promotion and social networking platform for digital designers and creatives. Land-book - design gallery with the best and most carefully collected websites. We help creatives find inspiration \u0026amp; motivation to do rad stuff. awwwards - are the Website Awards that recognize and promote the talent and effort of the best developers, designers and web agencies in the world.  ⇧ back to topIllustrations  Pexels - The best free stock photos \u0026amp; videos shared by talented creators. unDraw - Open-source illustrations for any idea you can imagine and create. DrawKit - Hand-drawn vector illustration and icon resources, perfect for your next project. freepik - Graphic resources for everyone. pixabay - Stunning free images \u0026amp; royalty free stock. icons8 - Free vector illustrations to class up your project.  ⇧ back to topPlaceholder  Placedog - Hand-picked, stunning photographs of our favourite pets: dogs! Free to use with your websites/projects. Placekitten - A quick and simple service for getting pictures of kittens for use as placeholders in your designs or code. Placeholder - is a free image placeholder service for web designers, serving billions and billions of images each year. Picsum - The Lorem Ipsum for photos.  Games🕹️  Grid Garden - A game where you write CSS code to grow your carrot garden. 🥕 Flexbox Froggy - A game where you help Froggy and friends by writing CSS code. 🐸 Flexbox Defense - Tower Defense with a twist: all towers must be positioned with CSS Flexbox. CSS Dinner - It\u0026rsquo;s a fun game to learn and practice CSS selectors.  ⇧ back to topIcons  Font Awesome - Get vector icons and social logos on your website with Font Awesome, the web\u0026rsquo;s most popular icon set and toolkit. Ikonate - Ikonate are fully customisable \u0026amp; accessible*, well-optimised vector icons. LineIcons - Handcrafted Line Icons for Modern User Interfaces of Web, Android, iOS, and Desktop App Projects. TheNounProject - Icons and Photos For Everything. MaterialDesignIcons - Beautifully crafted symbols for common actions and items. FlatIcon - The best UI icons for your projects. IconFinder - is the world\u0026rsquo;s largest marketplace for vector and raster icons in SVG and PNG formats. FeatherIcons - is a collection of simply beautiful open source icons. Each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency and readability. StreamLineIcons - The world’s largest icon pack. LinearIcons - is the highest quality set of line icons, matching with minimalist UI designs in iOS. Unicons - Web’s new favorite icon library. NucleoApp - Nucleo is a beautiful library of 30635 icons, and a powerful application to collect, customize and export all your icons. CoreUI Icons - Premium designed free icon set with marks in SVG, Webfont and raster formats. Line-Awesome - Swap Font Awesome for modern line icons in one line of code.  ⇧ back to topMore coming soon! Stay tuned\u0026hellip;\n","permalink":"https://elvisfinol.github.io/blog/posts/css-tools/","summary":"Frameworks, guides and several tools to help you improve your writing CSS.\nTable content  Frameworks Tools Preprocessors Reset and Normalize Design Inspiration Illustrations Placeholder Games Icons  Frameworks🎨 CSS framework gives web developers a basic structure, which includes grid, interactive UI patterns, web typography, tooltips, buttons, form elements, icons. This structure helps web developers to start quickly and efficiently when they are designing a website or web applications.\n awsm.","title":"CSS Tools \u0026 Resources🌈"},{"content":"","permalink":"https://elvisfinol.github.io/blog/archive/","summary":"archives","title":"Blog"},{"content":"","permalink":"https://elvisfinol.github.io/blog/search/","summary":"search","title":"Search"},{"content":"¡Hola! Mi nombre es Elvis Finol. Nací hace 29 años en Maracaibo (Venezuela). Estudié Ingeniería en Electrónica Mención Automatización y Control de Procesos en la Universidad Rafael Belloso Chacín. En el 2015 decidí salir de mi país en busca de mejores oportunidades tanto para el crecimiento personal como profesional. Me mudé a la capital Argentina, la icónica Ciudad de Buenos Aires.\n Marzo de 2021 \u0026ndash;\u0026gt; nueva localización / Madrid, España. (Historia en desarrollo\u0026hellip;✍️)\n A pensar que mi título diga lo contrario, desde siempre me he desarrollado en al área de Sistemas. He extendido mis conocimientos en una amplia variedad de entornos (Infraestructura on Premise y Cloud). Día a día me enfrento a la resolución de problemas y a mantener tanto sistemas Productivos como DR; participando en proyectos que van desde la instalación de infraestructura, el desarrollo, las pruebas, hasta implementación y liberación de procesos productivos. Esto me ha llevado a generar una gran habilidad de trabajo bajo presión.\n Experiencia en Datacenter Infrastructure, Networking, Public Cloud (AWS), Data Warehousing (Teradata), Project Management.\n Alguno de los proyectos recientes que he participado puedo mencionar los siguientes:\nTeradata\n Banco Central De La República Argentina – Floor sweep y migración a Teradata 16.20 incluyendo la implementación de nueva solución BAR/Librería (2020) HSBC - Instalación/migración de nueva plataforma Intellibase DEVUAT (2019) Prisma Medios De Pagos – Gerente de proyecto para Customer Services en Floor sweep y migración a Teradata 16.20 Telefónica Argentina – Floor sweep y migración a Teradata 16.20, trabajando como Gerente de proyecto para Customer Services. (2019) Banco Galicia – Líder para migración de toda la infraestructura de Backup y Restore, incluyendo implementación UDA (Unified Data architecture) (2018) Carrefour (retail) – Líder para expansión de Teradata (Merge) llevando sistema 6700 2+1 a 6800 1+1 (2017)   Además este último año me he estado formando, aprendiendo tecnologías Front-end para el Desarrollo Web (HTML, CSS y JavaScript) y he realizado algunos trabajos Freelance que actualmente tengo disponibles en mi GitHub.\n Un poco mas de mi - Hobbies! 💥 Pasión por la música Desde muy chico descubrí que me encantaba la música. Recuerdo las visitas a las tiendas de discos donde guardaba los pocos ahorros que tenía para comprar los últimos álbumes (aquellos tiempos). No fue hasta los 12 años que mi papá me regalo una guitarra española y ahí no pare de aprender y hacer música. Aprendí a tocar el bajo eléctrico, guitarra, teclado y lo que más disfruto es la producción; estar detrás de backstage, en mi espacio creando música. Te dejo este enlace de mis producciones en el área de la música.\nContenido Visual y Fotografía En el 2019, Mica y yo decidimos crear Vive Simple un espacio en Youtube e Instagram en el que ambos pudiéramos canalizar nuestra creatividad, compartiendo nuestras experiencias. Todo esto con el fin de ayudar a las personas y darles a conocer lo hermoso que es Buenos Aires, Argentina.\n¡Escríbeme! 📥 No dudes en contactarme si consideras que puedo ser de valor en tu negocio. ¡Nos vemos y gracias por visitar este espacio!\n","permalink":"https://elvisfinol.github.io/blog/sobremi/","summary":"sobremi","title":"Sobre Mi"}]